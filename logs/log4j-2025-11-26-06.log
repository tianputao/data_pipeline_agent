25/11/26 06:53:25 INFO DriverDaemon$: Started Log4j2
25/11/26 06:53:27 INFO DriverDaemon$: Current JVM Version 1.8.0_462
25/11/26 06:53:27 INFO DriverDaemon$: ========== driver starting up ==========
25/11/26 06:53:27 INFO DriverDaemon$: Java: Azul Systems, Inc. 1.8.0_462
25/11/26 06:53:27 INFO DriverDaemon$: OS: Linux/amd64 5.15.0-1091-azure
25/11/26 06:53:27 INFO DriverDaemon$: CWD: /databricks/driver
25/11/26 06:53:27 INFO DriverDaemon$: Mem: Max: 7.9G loaded GCs: PS Scavenge, PS MarkSweep
25/11/26 06:53:27 INFO DriverDaemon$: Logging multibyte characters: âœ“
25/11/26 06:53:27 INFO DriverDaemon$: 'publicFile.rolling.rewrite' appender in root logger: class org.apache.logging.log4j.core.appender.rewrite.RewriteAppender
25/11/26 06:53:27 INFO DriverDaemon$: == Modules:
25/11/26 06:53:28 INFO DriverDaemon$: Starting prometheus metrics log export timer
25/11/26 06:53:28 INFO DriverConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:28 INFO DriverConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:28 WARN DriverConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value
25/11/26 06:53:29 INFO DriverDaemon$: Loaded JDBC drivers in 52 ms
25/11/26 06:53:29 INFO DriverDaemon$: Universe Git Hash: f574797ba68fce84d3a50e05c13993f77f57aea0
25/11/26 06:53:29 INFO DriverDaemon$: Spark Git Hash: d0b1371a97d80366f49e5e050bac9e7922e31fd0
25/11/26 06:53:29 WARN SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.password
25/11/26 06:53:29 WARN SparkConfUtils$: Setting the same key twice for spark.databricks.io.directoryCommit.enableLogicalDelete
25/11/26 06:53:29 WARN SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.path
25/11/26 06:53:29 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
25/11/26 06:53:29 WARN RunHelpers$: Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.,DATA_LABEL_SYSTEM_NOT_SENSITIVE,false,false,List(),UsageLogRedactionConfig(List()))
25/11/26 06:53:29 INFO DatabricksILoop$: Creating throwaway interpreter
25/11/26 06:53:29 INFO MetastoreMonitor$: Internal metastore configured
25/11/26 06:53:29 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-chinanorth3-prod-metastore-0.mysql.database.chinacloudapi.cn:3306/organization2559323315997869?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
25/11/26 06:53:29 INFO NestedConnectionMonitor$$anon$1: Configured feature flag data source LaunchDarkly
25/11/26 06:53:29 INFO NestedConnectionMonitor$$anon$1: Configured feature flag data source LaunchDarkly
25/11/26 06:53:29 WARN NestedConnectionMonitor$$anon$1: REGION environment variable is not defined. getConfForCurrentRegion will always return default value
25/11/26 06:53:29 INFO FeatureFlagRegisterConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:29 INFO FeatureFlagRegisterConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:29 WARN FeatureFlagRegisterConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value
25/11/26 06:53:29 INFO DriverCorral: Creating the driver context
25/11/26 06:53:29 INFO DatabricksILoop$: Class Server Dir: /local_disk0/tmp/repl/spark-2123861716830151965-4f8b061d-55bb-4069-86a4-df36cd728eca
25/11/26 06:53:29 INFO HikariDataSource: metastore-monitor - Starting...
25/11/26 06:53:29 INFO HikariDataSource: metastore-monitor - Start completed.
25/11/26 06:53:29 WARN SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.password
25/11/26 06:53:29 WARN SparkConfUtils$: Setting the same key twice for spark.databricks.io.directoryCommit.enableLogicalDelete
25/11/26 06:53:29 WARN SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.path
25/11/26 06:53:29 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
25/11/26 06:53:29 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
25/11/26 06:53:29 INFO DynamicRpcConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:29 INFO DynamicRpcConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:29 WARN DynamicRpcConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value
25/11/26 06:53:30 INFO SparkContext: Running Spark version 3.3.2
25/11/26 06:53:30 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
25/11/26 06:53:30 INFO HikariDataSource: metastore-monitor - Shutdown completed.
25/11/26 06:53:30 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 1264 milliseconds)
25/11/26 06:53:30 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-chinanorth3-prod-metastore-0.mysql.database.chinacloudapi.cn:9207/organization2559323315997869?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
25/11/26 06:53:30 INFO HikariDataSource: metastore-monitor - Starting...
25/11/26 06:53:30 INFO HikariDataSource: metastore-monitor - Start completed.
25/11/26 06:53:30 INFO ResourceUtils: ==============================================================
25/11/26 06:53:30 INFO ResourceUtils: No custom resources configured for spark.driver.
25/11/26 06:53:30 INFO ResourceUtils: ==============================================================
25/11/26 06:53:30 INFO SparkContext: Submitted application: Databricks Shell
25/11/26 06:53:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 8874, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/11/26 06:53:30 INFO ResourceProfile: Limiting resource is cpu
25/11/26 06:53:30 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/11/26 06:53:30 INFO SecurityManager: Changing view acls to: root
25/11/26 06:53:30 INFO SecurityManager: Changing modify acls to: root
25/11/26 06:53:30 INFO SecurityManager: Changing view acls groups to: 
25/11/26 06:53:30 INFO SecurityManager: Changing modify acls groups to: 
25/11/26 06:53:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set(); RPC SSL disabled
25/11/26 06:53:30 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
25/11/26 06:53:31 INFO HikariDataSource: metastore-monitor - Shutdown completed.
25/11/26 06:53:31 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 511 milliseconds)
25/11/26 06:53:31 INFO Utils: Successfully started service 'sparkDriver' on port 41239.
25/11/26 06:53:31 INFO SparkEnv: Registering MapOutputTracker
25/11/26 06:53:31 INFO SecurityManager: Changing view acls to: root
25/11/26 06:53:31 INFO SecurityManager: Changing modify acls to: root
25/11/26 06:53:31 INFO SecurityManager: Changing view acls groups to: 
25/11/26 06:53:31 INFO SecurityManager: Changing modify acls groups to: 
25/11/26 06:53:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set(); RPC SSL disabled
25/11/26 06:53:31 INFO SparkEnv: Registering BlockManagerMaster
25/11/26 06:53:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/11/26 06:53:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/11/26 06:53:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/11/26 06:53:31 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-92ce8793-4691-4a75-9dc3-0b73b23a65b9
25/11/26 06:53:31 INFO MemoryStore: MemoryStore started with capacity 4.4 GiB
25/11/26 06:53:31 INFO SparkEnv: Registering OutputCommitCoordinator
25/11/26 06:53:31 INFO SparkContext: Spark configuration:
libraryDownload.sleepIntervalSeconds=5
libraryDownload.timeoutSeconds=180
spark.akka.frameSize=256
spark.app.name=Databricks Shell
spark.app.startTime=1764140010257
spark.cleaner.referenceTracking.blocking=false
spark.databricks.SC200982.blockBuiltInFunctionOverride=true
spark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient
spark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider
spark.databricks.acl.scim.client=com.databricks.spark.sql.acl.client.DriverToWebappScimClient
spark.databricks.automl.serviceEnabled=true
spark.databricks.autotune.maintenance.client.classname=com.databricks.maintenanceautocompute.MACClientImpl
spark.databricks.cloudProvider=Azure
spark.databricks.cloudfetch.hasRegionSupport=true
spark.databricks.cloudfetch.requestDownloadUrlsWithHeaders=*********(redacted)
spark.databricks.cloudfetch.requesterClassName=*********(redacted)
spark.databricks.clusterSource=UI
spark.databricks.clusterUsageTags.attribute_tag_budget=
spark.databricks.clusterUsageTags.attribute_tag_dust_bazel_path=
spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env=
spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer=
spark.databricks.clusterUsageTags.attribute_tag_dust_resource_class=
spark.databricks.clusterUsageTags.attribute_tag_dust_runbot_id=
spark.databricks.clusterUsageTags.attribute_tag_dust_runner=
spark.databricks.clusterUsageTags.attribute_tag_dust_suite=
spark.databricks.clusterUsageTags.attribute_tag_platform_name=
spark.databricks.clusterUsageTags.attribute_tag_service=
spark.databricks.clusterUsageTags.autoTerminationMinutes=60
spark.databricks.clusterUsageTags.azureSubscriptionId=8cf1045d-f235-4fd3-beff-c67785d438b1
spark.databricks.clusterUsageTags.cloudProvider=Azure
spark.databricks.clusterUsageTags.clusterAllTags=[{"key":"Vendor","value":"Databricks"},{"key":"Creator","value":"fantian@tianfan.partner.onmschina.cn"},{"key":"ClusterName","value":"ai_pipeline"},{"key":"ClusterId","value":"1125-113639-ndjs76hq"},{"key":"DatabricksEnvironment","value":"workerenv-2559323315997869"}]
spark.databricks.clusterUsageTags.clusterAvailability=ON_DEMAND_AZURE
spark.databricks.clusterUsageTags.clusterCreator=Webapp
spark.databricks.clusterUsageTags.clusterFirstOnDemand=1
spark.databricks.clusterUsageTags.clusterGeneration=4
spark.databricks.clusterUsageTags.clusterId=1125-113639-ndjs76hq
spark.databricks.clusterUsageTags.clusterLastActivityTime=1764139715713
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=false
spark.databricks.clusterUsageTags.clusterLogDestination=
spark.databricks.clusterUsageTags.clusterLogDestinationType=
spark.databricks.clusterUsageTags.clusterMaxWorkers=8
spark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT
spark.databricks.clusterUsageTags.clusterMinWorkers=1
spark.databricks.clusterUsageTags.clusterName=ai_pipeline
spark.databricks.clusterUsageTags.clusterNoDriverDaemon=false
spark.databricks.clusterUsageTags.clusterNodeType=Standard_D4ds_v5
spark.databricks.clusterUsageTags.clusterNodeTypeFlexibilityEnabled=false
spark.databricks.clusterUsageTags.clusterNumCustomTags=0
spark.databricks.clusterUsageTags.clusterNumSshKeys=0
spark.databricks.clusterUsageTags.clusterOwnerOrgId=2559323315997869
spark.databricks.clusterUsageTags.clusterOwnerUserId=*********(redacted)
spark.databricks.clusterUsageTags.clusterPinned=false
spark.databricks.clusterUsageTags.clusterPythonVersion=3
spark.databricks.clusterUsageTags.clusterResourceClass=default
spark.databricks.clusterUsageTags.clusterScalingType=autoscaling
spark.databricks.clusterUsageTags.clusterSizeType=VM_CONTAINER
spark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU
spark.databricks.clusterUsageTags.clusterSpotBidMaxPrice=-1.0
spark.databricks.clusterUsageTags.clusterState=Restarting
spark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark
spark.databricks.clusterUsageTags.clusterTargetWorkers=1
spark.databricks.clusterUsageTags.clusterUnityCatalogMode=*********(redacted)
spark.databricks.clusterUsageTags.clusterWorkers=1
spark.databricks.clusterUsageTags.computeKind=CLASSIC_PREVIEW
spark.databricks.clusterUsageTags.containerType=LXC
spark.databricks.clusterUsageTags.dataPlaneRegion=chinanorth3
spark.databricks.clusterUsageTags.driverContainerId=44e2732adecc4b9b93ef1e2854e94c28
spark.databricks.clusterUsageTags.driverContainerPrivateIp=10.139.64.5
spark.databricks.clusterUsageTags.driverInstanceId=2a145075d1af445bb31ad568cbdafffe
spark.databricks.clusterUsageTags.driverInstancePrivateIp=10.139.0.4
spark.databricks.clusterUsageTags.driverNodeType=Standard_D4ds_v5
spark.databricks.clusterUsageTags.driverPublicDns=52.130.161.173
spark.databricks.clusterUsageTags.effectiveSparkVersion=12.2.x-scala2.12
spark.databricks.clusterUsageTags.enableCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableDfAcls=false
spark.databricks.clusterUsageTags.enableElasticDisk=true
spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableJdbcAutoStart=true
spark.databricks.clusterUsageTags.enableJobsAutostart=true
spark.databricks.clusterUsageTags.enableLocalDiskEncryption=false
spark.databricks.clusterUsageTags.enableSqlAclsOnly=false
spark.databricks.clusterUsageTags.hailEnabled=false
spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting=false
spark.databricks.clusterUsageTags.instanceWorkerEnvId=workerenv-2559323315997869
spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType=default
spark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled=false
spark.databricks.clusterUsageTags.isGroupCluster=false
spark.databricks.clusterUsageTags.isIMv2Enabled=true
spark.databricks.clusterUsageTags.isServicePrincipalCluster=false
spark.databricks.clusterUsageTags.isSingleNode=false
spark.databricks.clusterUsageTags.isSingleUserCluster=*********(redacted)
spark.databricks.clusterUsageTags.managedResourceGroup=databricks-rg-Databricks-cn3-prod-52ba56scwuu2k
spark.databricks.clusterUsageTags.ngrokNpipEnabled=false
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=0
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss=0
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs=0
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File=0
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs=0
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3=0
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Volumes=0
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Workspace=0
spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2=0
spark.databricks.clusterUsageTags.orgId=2559323315997869
spark.databricks.clusterUsageTags.privateLinkEnabled=false
spark.databricks.clusterUsageTags.region=chinanorth3
spark.databricks.clusterUsageTags.runtimeEngine=STANDARD
spark.databricks.clusterUsageTags.shardName=az-chinanorth2
spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes=false
spark.databricks.clusterUsageTags.sparkImageLabel=release__12.2.x-snapshot-scala2.12__databricks__12.2.62__f574797__d0b1371__jenkins__b6e50ce__format-3
spark.databricks.clusterUsageTags.sparkMasterUrlType=*********(redacted)
spark.databricks.clusterUsageTags.sparkVersion=12.2.x-scala2.12
spark.databricks.clusterUsageTags.userId=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedSparkVersion=*********(redacted)
spark.databricks.clusterUsageTags.workerEnvironmentId=workerenv-2559323315997869
spark.databricks.credential.aws.secretKey.redactor=*********(redacted)
spark.databricks.credential.redactor=*********(redacted)
spark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName=*********(redacted)
spark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName=*********(redacted)
spark.databricks.credential.scope.fs.impl=*********(redacted)
spark.databricks.credential.scope.fs.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.delta.logStore.crossCloud.fatal=true
spark.databricks.delta.multiClusterWrites.enabled=true
spark.databricks.deltaSharing.clientClassName=com.databricks.deltasharing.DataSharingClientImpl
spark.databricks.driver.cleanUpSparkSessionsOnUCSharedClusters=true
spark.databricks.driver.enableDncOomMessage=false
spark.databricks.driver.preferredMavenCentralMirrorUrl=*********(redacted)
spark.databricks.driverNfs.clusterWidePythonLibsEnabled=true
spark.databricks.driverNfs.enabled=true
spark.databricks.driverNfs.pathSuffix=.ephemeral_nfs
spark.databricks.driverNodeTypeId=Standard_D4ds_v5
spark.databricks.enablePublicDbfsFuse=false
spark.databricks.eventLog.dir=eventlogs
spark.databricks.eventLog.enabled=true
spark.databricks.eventLog.listenerClassName=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.databricks.instanceId=2a145075d1af445bb31ad568cbdafffe
spark.databricks.io.cache.initialDiskSize=161061273600
spark.databricks.io.directoryCommit.enableLogicalDelete=false
spark.databricks.isShieldWorkspace=false
spark.databricks.managedCatalog.clientClassName=com.databricks.managedcatalog.ManagedCatalogClientImpl
spark.databricks.metrics.filesystem_io_metrics=true
spark.databricks.mlflow.autologging.enableGenAIFlavors=true
spark.databricks.mlflow.autologging.enabled=true
spark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
spark.databricks.passthrough.adls.gen2.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.adls.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.glue.credentialsProviderFactoryClassName=*********(redacted)
spark.databricks.passthrough.glue.executorServiceFactoryClassName=*********(redacted)
spark.databricks.passthrough.oauth.refresher.impl=*********(redacted)
spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class=com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory
spark.databricks.passthrough.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.preemption.enabled=true
spark.databricks.privateLinkEnabled=false
spark.databricks.proxyHadoopTraffic.host=storage-proxy.databricks.com
spark.databricks.proxyHadoopTraffic.port=9210
spark.databricks.python.defaultPythonRepl=ipykernel
spark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy
spark.databricks.repl.enableClassFileCleanup=true
spark.databricks.safer.unifiedPath.applyFlag.enabled=true
spark.databricks.secret.envVar.keys.toRedact=*********(redacted)
spark.databricks.secret.sparkConf.keys.toRedact=*********(redacted)
spark.databricks.service.dbutils.repl.backend=com.databricks.dbconnect.ReplDBUtils
spark.databricks.service.dbutils.server.backend=com.databricks.dbconnect.SparkServerDBUtils
spark.databricks.session.share=false
spark.databricks.sparkContextId=2123861716830151965
spark.databricks.sql.configMapperClass=com.databricks.dbsql.config.SqlConfigMapperBridge
spark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.MultiClusterLogStore
spark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore
spark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore
spark.databricks.tahoe.logStore.gcp.class=com.databricks.tahoe.store.GCPLogStore
spark.databricks.telemetry.prometheus.samplingRate=100
spark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName=*********(redacted)
spark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled=*********(redacted)
spark.databricks.unityCatalog.enabled=true
spark.databricks.unityCatalog.enforce.permissions=false
spark.databricks.unityCatalog.externalLocationFallbackMode.enabled=false
spark.databricks.unityCatalog.volumes.enabled=true
spark.databricks.workerNodeTypeId=Standard_D4ds_v5
spark.databricks.workspaceUrl=*********(redacted)
spark.databricks.wsfs.workspacePrivatePreview=true
spark.databricks.wsfsPublicPreview=true
spark.delta.sharing.profile.provider.class=*********(redacted)
spark.driver.allowMultipleContexts=false
spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED
spark.driver.host=10.139.64.5
spark.driver.maxResultSize=4g
spark.driver.port=41239
spark.driver.tempDirectory=/local_disk0/tmp
spark.eventLog.enabled=false
spark.executor.extraClassPath=/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*
spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1
spark.executor.id=driver
spark.executor.memory=8874m
spark.executor.tempDirectory=/local_disk0/tmp
spark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.files.fetchFailure.unRegisterOutputOnHost=true
spark.files.overwrite=true
spark.files.useFetchCache=false
spark.hadoop.databricks.dbfs.client.version=v2
spark.hadoop.databricks.fs.perfMetrics.enable=true
spark.hadoop.databricks.loki.fileStatusCache.abfs.enabled=false
spark.hadoop.databricks.loki.fileStatusCache.gcs.enabled=false
spark.hadoop.databricks.loki.fileStatusCache.s3a.enabled=false
spark.hadoop.databricks.loki.fileSystemCache.enabled=true
spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories=false
spark.hadoop.databricks.s3.verifyBucketExists.enabled=false
spark.hadoop.databricks.s3commit.client.sslTrustAll=false
spark.hadoop.fs.AbstractFileSystem.gs.impl=shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS
spark.hadoop.fs.abfs.impl=com.databricks.common.filesystem.LokiFileSystem
spark.hadoop.fs.abfs.impl.disable.cache=true
spark.hadoop.fs.abfss.impl=com.databricks.common.filesystem.LokiFileSystem
spark.hadoop.fs.abfss.impl.disable.cache=true
spark.hadoop.fs.adl.impl=com.databricks.adl.AdlFileSystem
spark.hadoop.fs.adl.impl.disable.cache=true
spark.hadoop.fs.azure.authorization.caching.enable=false
spark.hadoop.fs.azure.cache.invalidator.type=com.databricks.encryption.utils.CacheInvalidatorImpl
spark.hadoop.fs.azure.skip.metrics=true
spark.hadoop.fs.azure.user.agent.prefix=*********(redacted)
spark.hadoop.fs.cpfs-abfss.impl=*********(redacted)
spark.hadoop.fs.cpfs-abfss.impl.disable.cache=true
spark.hadoop.fs.cpfs-adl.impl=*********(redacted)
spark.hadoop.fs.cpfs-adl.impl.disable.cache=true
spark.hadoop.fs.cpfs-s3.impl=*********(redacted)
spark.hadoop.fs.cpfs-s3a.impl=*********(redacted)
spark.hadoop.fs.cpfs-s3n.impl=*********(redacted)
spark.hadoop.fs.dbfs.impl=com.databricks.backend.daemon.data.client.DbfsHadoop3
spark.hadoop.fs.dbfsartifacts.impl=com.databricks.backend.daemon.data.client.DBFSV1
spark.hadoop.fs.fcfs-abfs.impl=*********(redacted)
spark.hadoop.fs.fcfs-abfs.impl.disable.cache=true
spark.hadoop.fs.fcfs-abfss.impl=*********(redacted)
spark.hadoop.fs.fcfs-abfss.impl.disable.cache=true
spark.hadoop.fs.fcfs-s3.impl=*********(redacted)
spark.hadoop.fs.fcfs-s3.impl.disable.cache=true
spark.hadoop.fs.fcfs-s3a.impl=*********(redacted)
spark.hadoop.fs.fcfs-s3a.impl.disable.cache=true
spark.hadoop.fs.fcfs-s3n.impl=*********(redacted)
spark.hadoop.fs.fcfs-s3n.impl.disable.cache=true
spark.hadoop.fs.fcfs-wasb.impl=*********(redacted)
spark.hadoop.fs.fcfs-wasb.impl.disable.cache=true
spark.hadoop.fs.fcfs-wasbs.impl=*********(redacted)
spark.hadoop.fs.fcfs-wasbs.impl.disable.cache=true
spark.hadoop.fs.file.impl=com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem
spark.hadoop.fs.gs.impl=com.databricks.common.filesystem.LokiFileSystem
spark.hadoop.fs.gs.impl.disable.cache=true
spark.hadoop.fs.gs.outputstream.upload.chunk.size=16777216
spark.hadoop.fs.idbfs.impl=com.databricks.io.idbfs.IdbfsFileSystem
spark.hadoop.fs.mlflowdbfs.impl=com.databricks.mlflowdbfs.MlflowdbfsFileSystem
spark.hadoop.fs.s3.impl=com.databricks.common.filesystem.LokiFileSystem
spark.hadoop.fs.s3.impl.disable.cache=true
spark.hadoop.fs.s3a.assumed.role.credentials.provider=*********(redacted)
spark.hadoop.fs.s3a.attempts.maximum=10
spark.hadoop.fs.s3a.block.size=67108864
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.connection.timeout=50000
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.active.blocks=32
spark.hadoop.fs.s3a.fast.upload.default=true
spark.hadoop.fs.s3a.impl=com.databricks.common.filesystem.LokiFileSystem
spark.hadoop.fs.s3a.impl.disable.cache=true
spark.hadoop.fs.s3a.max.total.tasks=1000
spark.hadoop.fs.s3a.multipart.size=10485760
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.retry.interval=250ms
spark.hadoop.fs.s3a.retry.limit=6
spark.hadoop.fs.s3a.retry.throttle.interval=500ms
spark.hadoop.fs.s3a.threads.max=136
spark.hadoop.fs.s3n.impl=com.databricks.common.filesystem.LokiFileSystem
spark.hadoop.fs.s3n.impl.disable.cache=true
spark.hadoop.fs.stage.impl=com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem
spark.hadoop.fs.stage.impl.disable.cache=true
spark.hadoop.fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasb.impl.disable.cache=true
spark.hadoop.fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasbs.impl.disable.cache=true
spark.hadoop.hive.hmshandler.retry.attempts=10
spark.hadoop.hive.hmshandler.retry.interval=2000
spark.hadoop.hive.server2.enable.doAs=false
spark.hadoop.hive.server2.idle.operation.timeout=7200000
spark.hadoop.hive.server2.idle.session.timeout=900000
spark.hadoop.hive.server2.keystore.password=*********(redacted)
spark.hadoop.hive.server2.keystore.path=/databricks/keys/jetty-ssl-driver-keystore.jks
spark.hadoop.hive.server2.session.check.interval=60000
spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled=false
spark.hadoop.hive.server2.thrift.http.port=10000
spark.hadoop.hive.server2.transport.mode=http
spark.hadoop.hive.server2.use.SSL=true
spark.hadoop.hive.warehouse.subdir.inherit.perms=false
spark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.parquet.abfs.readahead.optimization.enabled=true
spark.hadoop.parquet.block.size.row.check.max=10
spark.hadoop.parquet.block.size.row.check.min=10
spark.hadoop.parquet.filter.columnindex.enabled=false
spark.hadoop.parquet.memory.pool.ratio=0.5
spark.hadoop.parquet.page.metadata.validation.enabled=true
spark.hadoop.parquet.page.size.check.estimate=false
spark.hadoop.parquet.page.verify-checksum.enabled=true
spark.hadoop.parquet.page.write-checksum.enabled=true
spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled=false
spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException=false
spark.hadoop.spark.driverproxy.customHeadersToProperties=*********(redacted)
spark.hadoop.spark.hadoop.aws.glue.cache.db.size=1000
spark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins=30
spark.hadoop.spark.hadoop.aws.glue.cache.table.size=1000
spark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins=30
spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.home=/databricks/spark
spark.logConf=true
spark.master=spark://10.139.64.5:7077
spark.metrics.conf=/databricks/spark/conf/metrics.properties
spark.r.backendConnectionTimeout=604800
spark.r.numRBackendThreads=1
spark.rdd.compress=true
spark.repl.class.outputDir=/local_disk0/tmp/repl/spark-2123861716830151965-4f8b061d-55bb-4069-86a4-df36cd728eca
spark.rpc.message.maxSize=256
spark.scheduler.listenerbus.eventqueue.capacity=20000
spark.scheduler.mode=FAIR
spark.serializer.objectStreamReset=100
spark.shuffle.manager=SORT
spark.shuffle.memoryFraction=0.2
spark.shuffle.reduceLocality.enabled=false
spark.shuffle.service.enabled=true
spark.shuffle.service.port=4048
spark.sparklyr-backend.threads=1
spark.sparkr.use.daemon=false
spark.speculation=false
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.sql.allowMultipleContexts=false
spark.sql.hive.convertCTAS=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.metastore.jars=/databricks/databricks-hive/*
spark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks
spark.sql.hive.metastore.version=0.13.0
spark.sql.legacy.createHiveTableByDefault=false
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol
spark.sql.sources.default=delta
spark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager
spark.sql.streaming.stopTimeout=15s
spark.sql.warehouse.dir=*********(redacted)
spark.storage.blockManagerTimeoutIntervalMs=300000
spark.storage.memoryFraction=0.5
spark.streaming.driver.writeAheadLog.allowBatching=true
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=true
spark.task.reaper.enabled=true
spark.task.reaper.killTimeout=60s
spark.ui.port=40001
spark.ui.prometheus.enabled=true
spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass=com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient
spark.worker.aioaLazyConfig.iamReadinessCheckClientClass=com.databricks.backend.daemon.driver.NephosIamRoleCheckClient
spark.worker.cleanup.enabled=false
25/11/26 06:53:31 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
25/11/26 06:53:31 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3. Previous value: com.databricks.common.filesystem.LokiFileSystem
25/11/26 06:53:31 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3a. Previous value: com.databricks.common.filesystem.LokiFileSystem
25/11/26 06:53:31 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3n. Previous value: com.databricks.common.filesystem.LokiFileSystem
25/11/26 06:53:31 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme abfs. Previous value: com.databricks.common.filesystem.LokiFileSystem
25/11/26 06:53:31 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme abfss. Previous value: com.databricks.common.filesystem.LokiFileSystem
25/11/26 06:53:31 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme gs. Previous value: com.databricks.common.filesystem.LokiFileSystem
25/11/26 06:53:32 INFO log: Logging initialized @16458ms to org.eclipse.jetty.util.log.Slf4jLog
25/11/26 06:53:32 INFO Server: jetty-9.4.46.v20220331; built: 2022-03-31T16:38:08.030Z; git: bc17a0369a11ecf40bb92c839b9ef0a8ac50ea18; jvm 1.8.0_462-b08
25/11/26 06:53:32 INFO Server: Started @16709ms
25/11/26 06:53:32 INFO AbstractConnector: Started ServerConnector@641bd93e{HTTP/1.1, (http/1.1)}{10.139.64.5:40001}
25/11/26 06:53:32 INFO Utils: Successfully started service 'SparkUI' on port 40001.
25/11/26 06:53:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7c75f3a8{/,null,AVAILABLE,@Spark}
25/11/26 06:53:33 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
25/11/26 06:53:33 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
25/11/26 06:53:33 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
25/11/26 06:53:33 INFO FairSchedulableBuilder: Fair scheduler configuration not found, created default pool: default, schedulingMode: FAIR, minShare: 0, weight: 1
25/11/26 06:53:34 INFO DatabricksEdgeConfigs: serverlessEnabled : false
25/11/26 06:53:34 INFO DatabricksEdgeConfigs: perfPackEnabled : false
25/11/26 06:53:34 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
25/11/26 06:53:34 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://10.139.64.5:7077...
25/11/26 06:53:34 INFO TransportClientFactory: Successfully created connection to /10.139.64.5:7077 after 102 ms (0 ms spent in bootstraps)
25/11/26 06:53:34 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20251126065334-0000
25/11/26 06:53:34 INFO TaskSchedulerImpl: Task preemption enabled.
25/11/26 06:53:34 INFO DatabricksILoop$: Finished creating throwaway interpreter
25/11/26 06:53:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39807.
25/11/26 06:53:34 INFO NettyBlockTransferService: Server created on 10.139.64.5:39807
25/11/26 06:53:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/11/26 06:53:34 INFO BlockManager: external shuffle service port = 4048
25/11/26 06:53:34 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20251126065334-0000/0 on worker-20251126065325-10.139.64.4-44021 (10.139.64.4:44021) with 4 core(s)
25/11/26 06:53:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20251126065334-0000/0 on hostPort 10.139.64.4:44021 with 4 core(s), 8.7 GiB RAM
25/11/26 06:53:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.5, 39807, None)
25/11/26 06:53:35 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.5:39807 with 4.4 GiB RAM, BlockManagerId(driver, 10.139.64.5, 39807, None)
25/11/26 06:53:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.5, 39807, None)
25/11/26 06:53:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.5, 39807, None)
25/11/26 06:53:35 INFO DBCEventLoggingListener: Initializing DBCEventLoggingListener (compressionEnabled=true)
25/11/26 06:53:35 INFO DBCEventLoggingListener: Logging events to eventlogs/2123861716830151965/eventlog
25/11/26 06:53:35 INFO SparkContext: Registered listener com.databricks.backend.daemon.driver.DBCEventLoggingListener
25/11/26 06:53:35 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20251126065334-0000/0 is now RUNNING
25/11/26 06:53:35 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3. Previous value: com.databricks.common.filesystem.LokiFileSystem
25/11/26 06:53:35 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3a. Previous value: com.databricks.common.filesystem.LokiFileSystem
25/11/26 06:53:35 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3n. Previous value: com.databricks.common.filesystem.LokiFileSystem
25/11/26 06:53:35 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme abfs. Previous value: com.databricks.common.filesystem.LokiFileSystem
25/11/26 06:53:35 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme abfss. Previous value: com.databricks.common.filesystem.LokiFileSystem
25/11/26 06:53:35 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme gs. Previous value: com.databricks.common.filesystem.LokiFileSystem
25/11/26 06:53:35 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@7c75f3a8{/,null,STOPPED,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4c6a5712{/jobs,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4cef1ea6{/jobs/json,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6a636c62{/jobs/job,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5863ef93{/jobs/job/json,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@519f6adb{/stages,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@22938166{/stages/json,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5c26ab0a{/stages/stage,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6ddd71cc{/stages/stage/json,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6258ea84{/stages/pool,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6a275836{/stages/pool/json,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3f41a1f3{/storage,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1342c6e1{/storage/json,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7828111d{/storage/rdd,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@184b8899{/storage/rdd/json,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4992e34f{/environment,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@49a6b730{/environment/json,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@33fa6a8b{/executors,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2d5cb059{/executors/json,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@431782fe{/executors/threadDump,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2680474c{/executors/threadDump/json,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@329cdafa{/executors/heapHistogram,null,AVAILABLE,@Spark}
25/11/26 06:53:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4f283b8f{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
25/11/26 06:53:36 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4b0a6c53{/static,null,AVAILABLE,@Spark}
25/11/26 06:53:36 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@57e0bfd6{/,null,AVAILABLE,@Spark}
25/11/26 06:53:36 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@12f826c{/api,null,AVAILABLE,@Spark}
25/11/26 06:53:36 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@746fda68{/metrics,null,AVAILABLE,@Spark}
25/11/26 06:53:36 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@74f23ecc{/jobs/job/kill,null,AVAILABLE,@Spark}
25/11/26 06:53:36 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1cbb6037{/stages/stage/kill,null,AVAILABLE,@Spark}
25/11/26 06:53:36 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@de0922c{/metrics/json,null,AVAILABLE,@Spark}
25/11/26 06:53:36 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/11/26 06:53:36 INFO SparkContext: Loading Spark Service RPC Server. Classloader stack:List(com.databricks.backend.daemon.driver.ClassLoaders$MultiReplClassLoader@24710e61, com.databricks.backend.daemon.driver.ClassLoaders$LibraryClassLoader@18f90bd8, sun.misc.Launcher$AppClassLoader@5aaa6d82, sun.misc.Launcher$ExtClassLoader@42b64ab8)
25/11/26 06:53:36 INFO SparkServiceRPCServer: Initializing Spark Service RPC Server. Classloader stack: List(com.databricks.backend.daemon.driver.ClassLoaders$MultiReplClassLoader@24710e61, com.databricks.backend.daemon.driver.ClassLoaders$LibraryClassLoader@18f90bd8, sun.misc.Launcher$AppClassLoader@5aaa6d82, sun.misc.Launcher$ExtClassLoader@42b64ab8)
25/11/26 06:53:36 INFO SparkServiceRPCServer: Starting Spark Service RPC Server
25/11/26 06:53:36 INFO SparkServiceRPCServer: Starting Spark Service RPC Server. Classloader stack: List(com.databricks.backend.daemon.driver.ClassLoaders$MultiReplClassLoader@24710e61, com.databricks.backend.daemon.driver.ClassLoaders$LibraryClassLoader@18f90bd8, sun.misc.Launcher$AppClassLoader@5aaa6d82, sun.misc.Launcher$ExtClassLoader@42b64ab8)
25/11/26 06:53:36 INFO Server: jetty-9.4.46.v20220331; built: 2022-03-31T16:38:08.030Z; git: bc17a0369a11ecf40bb92c839b9ef0a8ac50ea18; jvm 1.8.0_462-b08
25/11/26 06:53:36 INFO AbstractConnector: Started ServerConnector@65cfd565{HTTP/1.1, (http/1.1)}{0.0.0.0:15001}
25/11/26 06:53:36 INFO Server: Started @20693ms
25/11/26 06:53:36 INFO DatabricksILoop$: Successfully registered spark metrics in Prometheus registry
25/11/26 06:53:36 INFO DatabricksILoop$: Successfully initialized SparkContext
25/11/26 06:53:36 INFO SharedState: Scheduler stats enabled.
25/11/26 06:53:36 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/11/26 06:53:36 INFO SharedState: Warehouse path is 'dbfs:/user/hive/warehouse'.
25/11/26 06:53:36 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@66f73d3d{/storage/iocache,null,AVAILABLE,@Spark}
25/11/26 06:53:36 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@306cb0a0{/storage/iocache/json,null,AVAILABLE,@Spark}
25/11/26 06:53:36 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@125a6446{/SQL,null,AVAILABLE,@Spark}
25/11/26 06:53:36 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6852c3d2{/SQL/json,null,AVAILABLE,@Spark}
25/11/26 06:53:36 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2e6fa551{/SQL/execution,null,AVAILABLE,@Spark}
25/11/26 06:53:36 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@72722a6b{/SQL/execution/json,null,AVAILABLE,@Spark}
25/11/26 06:53:36 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6ca257de{/static/sql,null,AVAILABLE,@Spark}
25/11/26 06:53:36 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
25/11/26 06:53:36 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
25/11/26 06:53:40 INFO DriverConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:40 INFO DriverConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:40 WARN DriverConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value
25/11/26 06:53:40 INFO DriverConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:40 INFO DriverConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:40 WARN DriverConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value
25/11/26 06:53:41 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 166.639411 ms.
25/11/26 06:53:42 INFO DataClientConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:42 INFO DataClientConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:42 WARN DataClientConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value
25/11/26 06:53:42 INFO DataClientConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:42 INFO DataClientConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:42 WARN DataClientConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value
25/11/26 06:53:42 INFO DatabricksMountsStore: Mount store initialization: Attempting to get the list of mounts from metadata manager of DBFS
25/11/26 06:53:42 INFO log: Logging initialized @26842ms to shaded.v9_4.org.eclipse.jetty.util.log.Slf4jLog
25/11/26 06:53:42 INFO TypeUtil: JVM Runtime does not support Modules
25/11/26 06:53:43 INFO DatabricksMountsStore: Mount store initialization: Received a list of 9 mounts accessible from metadata manager of DBFS
25/11/26 06:53:43 INFO DatabricksMountsStore: Updated mounts cache. Changes: List((+,DbfsMountPoint(s3a://databricks-datasets-seoul/, /databricks-datasets)), (+,DbfsMountPoint(uc-volumes:/Volumes, /Volumes)), (+,DbfsMountPoint(unsupported-access-mechanism-for-path--use-mlflow-client:/, /databricks/mlflow-tracking)), (+,DbfsMountPoint(wasbs://dbstoragedyxwjqklqf7me.blob.core.chinacloudapi.cn/2559323315997869, /databricks-results)), (+,DbfsMountPoint(unsupported-access-mechanism-for-path--use-mlflow-client:/, /databricks/mlflow-registry)), (+,DbfsMountPoint(dbfs-reserved-path:/uc-volumes-reserved, /Volume)), (+,DbfsMountPoint(dbfs-reserved-path:/uc-volumes-reserved, /volumes)), (+,DbfsMountPoint(wasbs://dbstoragedyxwjqklqf7me.blob.core.chinacloudapi.cn/2559323315997869, /)), (+,DbfsMountPoint(dbfs-reserved-path:/uc-volumes-reserved, /volume)))
25/11/26 06:53:43 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.common.filesystem.LokiFileSystem.
25/11/26 06:53:43 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.common.filesystem.LokiFileSystem.
25/11/26 06:53:43 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.common.filesystem.LokiFileSystem.
25/11/26 06:53:43 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.common.filesystem.LokiFileSystem.
25/11/26 06:53:43 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.common.filesystem.LokiFileSystem.
25/11/26 06:53:43 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.common.filesystem.LokiFileSystem.
25/11/26 06:53:43 INFO DatabricksFileSystemV2Factory: Creating wasbs file system for wasbs://root@dbstoragedyxwjqklqf7me.blob.core.chinacloudapi.cn
25/11/26 06:53:43 INFO AzureNativeFileSystemStore: URI scheme: wasbs, using https for connections
25/11/26 06:53:43 INFO NativeAzureFileSystem: Delete with limit configurations: deleteFileCountLimitEnabled=false, deleteFileCountLimit=-1
25/11/26 06:53:43 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
25/11/26 06:53:43 INFO HiveConf: Found configuration file file:/databricks/hive/conf/hive-site.xml
25/11/26 06:53:44 INFO SessionManager: HiveServer2: Background operation thread pool size: 100
25/11/26 06:53:44 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100
25/11/26 06:53:44 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds
25/11/26 06:53:44 INFO AbstractService: Service:OperationManager is inited.
25/11/26 06:53:44 INFO AbstractService: Service:SessionManager is inited.
25/11/26 06:53:44 INFO SparkSQLCLIService: Service: CLIService is inited.
25/11/26 06:53:44 INFO AbstractService: Service:ThriftHttpCLIService is inited.
25/11/26 06:53:44 INFO HiveThriftServer2: Service: HiveServer2 is inited.
25/11/26 06:53:44 INFO AbstractService: Service:OperationManager is started.
25/11/26 06:53:44 INFO AbstractService: Service:SessionManager is started.
25/11/26 06:53:44 INFO SparkSQLCLIService: Service: CLIService is started.
25/11/26 06:53:44 INFO AbstractService: Service:ThriftHttpCLIService is started.
25/11/26 06:53:44 INFO ThriftCLIService: HTTP Server SSL: adding excluded protocols: [SSLv2, SSLv3]
25/11/26 06:53:44 INFO ThriftCLIService: HTTP Server SSL: SslContextFactory.getExcludeProtocols = [SSL, SSLv2, SSLv2Hello, SSLv3]
25/11/26 06:53:44 INFO Server: jetty-9.4.46.v20220331; built: 2022-03-31T16:38:08.030Z; git: bc17a0369a11ecf40bb92c839b9ef0a8ac50ea18; jvm 1.8.0_462-b08
25/11/26 06:53:44 INFO session: DefaultSessionIdManager workerName=node0
25/11/26 06:53:44 INFO session: No SessionScavenger set, using defaults
25/11/26 06:53:44 INFO session: node0 Scavenging every 660000ms
25/11/26 06:53:44 WARN SecurityHandler: ServletContext@o.e.j.s.ServletContextHandler@5022aaa3{/,null,STARTING} has uncovered http methods for path: /*
25/11/26 06:53:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5022aaa3{/,null,AVAILABLE}
25/11/26 06:53:44 INFO SslContextFactory: x509=X509@21e58e9a(1,h=[az-chinanorth2.workers.prod.ns.databricks.com],a=[],w=[]) for Server@453ea7f6[provider=null,keyStore=file:///databricks/keys/jetty-ssl-driver-keystore.jks,trustStore=null]
25/11/26 06:53:44 INFO AbstractConnector: Started ServerConnector@3bf1aba7{SSL, (ssl, http/1.1)}{0.0.0.0:10000}
25/11/26 06:53:44 INFO Server: Started @28818ms
25/11/26 06:53:44 INFO ThriftCLIService: Started ThriftHttpCLIService in https mode on port 10000 path=/cliservice/* with 5...500 worker threads
25/11/26 06:53:44 INFO AbstractService: Service:HiveServer2 is started.
25/11/26 06:53:44 INFO HiveThriftServer2: HiveThriftServer2 started
25/11/26 06:53:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@273cefd5{/sqlserver,null,AVAILABLE,@Spark}
25/11/26 06:53:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5b7bd551{/sqlserver/json,null,AVAILABLE,@Spark}
25/11/26 06:53:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@723c7f2f{/sqlserver/session,null,AVAILABLE,@Spark}
25/11/26 06:53:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@278758da{/sqlserver/session/json,null,AVAILABLE,@Spark}
25/11/26 06:53:44 INFO DatabricksILoop$: Trying to load dynamic config on startup: /databricks/driver/conf/dynamicSparkConfig.conf
25/11/26 06:53:44 INFO DatabricksILoop$: Read the dynamic config: ArrayBuffer(SaferConf(spark.databricks.dataLineage.mergeIntoV2Enabled,false,1731558697,241113190524268,1), SaferConf(spark.databricks.cloudFiles.aws.restrictSNSPolicyPrincipalAndAccount,false,1749573002,250606224725199,2), SaferConf(spark.databricks.unityCatalog.clientWithCaller.enabled,true,1753369006,250606151049532,1), SaferConf(spark.databricks.delta.merge.failSourceCachedAfterMaterialization,true,1763647692,251110154546851,1), SaferConf(spark.databricks.autotune.maintenance.enableMetricsForAllUCManagedTables,true,1756330524,250618183020206,1), SaferConf(spark.databricks.sql.jdbcDialectForbidQueriesWithForbiddenKeywords,true,1763644302,250811103850235,6), SaferConf(spark.databricks.unityCatalog.client.UCSEWithMessageTemplateAndClass.enabled,true,1763685074,250930185930488,2), SaferConf(spark.sql.optimizer.optimizeCsvJsonExprs.useSchemaField,false,1728499876,241008194704608,1), SaferConf(spark.databricks.delta.identifierExtraction.newCodePath.enabled,true,1747764696,250221192257794,3), SaferConf(spark.databricks.sql.inlineOuterRefsToConstants.enabled,false,1747429458,250319214018422,2), SaferConf(spark.databricks.autotune.maintenance.client.pushIntervalMinutes,1,1752168673,250118155345061,2), SaferConf(spark.databricks.unityCatalog.client.newErrorClasses.enabled,false,1755882681,250403002450986,1), SaferConf(spark.databricks.sql.jdbcDialectIgnoreQueriesWithForbiddenKeywords,false,1737662132,250123131150571,1), SaferConf(spark.databricks.delta.merge.disableSourceMaterializationNotAllowed,true,1761939217,250429083126347,2)) from /databricks/driver/conf/dynamicSparkConfig.conf, version 1764127249701
25/11/26 06:53:44 INFO LibraryResolutionManager: Preferred maven central mirror is configured to https://maven.aliyun.com/repository/central
25/11/26 06:53:44 WARN OutgoingDirectNotebookBufferRateLimiter$: No value specified for db-outgoing-buffer-throttler-burst. Using default: 100000000000
25/11/26 06:53:44 WARN OutgoingDirectNotebookBufferRateLimiter$: No value specified for db-outgoing-buffer-throttler-steady-rate. Using default: 6000000000
25/11/26 06:53:44 WARN OutgoingDirectNotebookBufferRateLimiter$: No value specified for db-outgoing-buffer-throttler-warning-interval-sec. Using default: 60
25/11/26 06:53:44 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/11/26 06:53:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@78373fdf{/StreamingQuery,null,AVAILABLE,@Spark}
25/11/26 06:53:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3a571436{/StreamingQuery/json,null,AVAILABLE,@Spark}
25/11/26 06:53:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5417f849{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
25/11/26 06:53:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7c57eaa6{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
25/11/26 06:53:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4d5665bb{/static/sql,null,AVAILABLE,@Spark}
25/11/26 06:53:44 INFO DriverCorral: metastoreType: InternalMysqlMetastore(DbMetastoreConfig{host=consolidated-chinanorth3-prod-metastore-0.mysql.database.chinacloudapi.cn, port=3306, dbName=organization2559323315997869, user=[REDACTED], proxyHost=consolidated-chinanorth3-prod-metastore-0.mysql.database.chinacloudapi.cn, proxyPort=9207, proxyUser=[REDACTED]}), enableMetastoreHealthCheck: true
25/11/26 06:53:44 INFO JettyServer$: Creating thread pool with name ...
25/11/26 06:53:44 INFO JettyServer$: Thread pool created
25/11/26 06:53:44 INFO JettyServer$: Creating thread pool with name ...
25/11/26 06:53:44 INFO JettyServer$: Thread pool created
25/11/26 06:53:44 INFO DriverDaemon: Starting driver daemon...
25/11/26 06:53:44 WARN SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.password
25/11/26 06:53:44 WARN SparkConfUtils$: Setting the same key twice for spark.databricks.io.directoryCommit.enableLogicalDelete
25/11/26 06:53:44 WARN SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.path
25/11/26 06:53:44 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
25/11/26 06:53:44 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
25/11/26 06:53:44 INFO DriverDaemon$: Attempting to run: 'set up ttyd daemon'
25/11/26 06:53:44 INFO DriverDaemon$: Attempting to run: 'Configuring RStudio daemon'
25/11/26 06:53:44 INFO DriverDaemon$: Resetting the default python executable
25/11/26 06:53:45 INFO Utils: resolved command to be run: List(virtualenv, /local_disk0/.ephemeral_nfs/cluster_libraries/python, -p, /databricks/python/bin/python, --no-download, --no-setuptools, --no-wheel)
25/11/26 06:53:45 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.139.64.4:60904) with ID 0, ResourceProfileId 0
25/11/26 06:53:46 INFO PythonEnvCloneHelper$: Created python virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
25/11/26 06:53:46 INFO Utils: resolved command to be run: List(/databricks/python/bin/python, -c, import sys; dirs=[p for p in sys.path if 'package' in p]; print(' '.join(dirs)))
25/11/26 06:53:46 INFO Utils: resolved command to be run: List(/local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/python, -c, from distutils.sysconfig import get_python_lib; print(get_python_lib()))
25/11/26 06:53:46 INFO PythonEnvCloneHelper$: Created sites.pth at /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/sites.pth
25/11/26 06:53:46 INFO ClusterWidePythonEnvManager: Registered /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages with the WatchService sun.nio.fs.LinuxWatchService$LinuxWatchKey@322733e
25/11/26 06:53:46 INFO DriverDaemon$: Attempting to run: 'Update root virtualenv'
25/11/26 06:53:46 INFO DriverDaemon$: Finished updating /etc/environment
25/11/26 06:53:46 INFO DriverDaemon$$anon$1: Thread to send message is ready
25/11/26 06:53:46 INFO NetstatUtil$: Running netstat -lnpt
25/11/26 06:53:46 INFO NetstatUtil$: netstat -lnpt
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:7681            0.0.0.0:*               LISTEN      781/ttyd            
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      47/systemd-resolved 
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      69/sshd: /usr/sbin/ 
tcp6       0      0 :::15001                :::*                    LISTEN      574/java            
tcp6       0      0 :::15002                :::*                    LISTEN      574/java            
tcp6       0      0 :::7071                 :::*                    LISTEN      332/java            
tcp6       0      0 10.139.64.5:41239       :::*                    LISTEN      574/java            
tcp6       0      0 :::6060                 :::*                    LISTEN      332/java            
tcp6       0      0 10.139.64.5:7077        :::*                    LISTEN      430/java            
tcp6       0      0 10.139.64.5:39807       :::*                    LISTEN      574/java            
tcp6       0      0 10.139.64.5:40000       :::*                    LISTEN      430/java            
tcp6       0      0 10.139.64.5:40001       :::*                    LISTEN      574/java            
tcp6       0      0 :::10000                :::*                    LISTEN      574/java            
tcp6       0      0 :::22                   :::*                    LISTEN      69/sshd: /usr/sbin/ 
tcp6       0      0 :::1021                 :::*                    LISTEN      168/wsfs            
tcp6       0      0 :::1017                 :::*                    LISTEN      168/wsfs            
tcp6       0      0 :::1015                 :::*                    LISTEN      199/goofys-dbr      

25/11/26 06:53:46 INFO Server: jetty-9.4.46.v20220331; built: 2022-03-31T16:38:08.030Z; git: bc17a0369a11ecf40bb92c839b9ef0a8ac50ea18; jvm 1.8.0_462-b08
25/11/26 06:53:46 INFO AbstractConnector: Started ServerConnector@87420c0{HTTP/1.1, (http/1.1)}{0.0.0.0:6061}
25/11/26 06:53:46 INFO Server: Started @31070ms
25/11/26 06:53:46 INFO Server: jetty-9.4.46.v20220331; built: 2022-03-31T16:38:08.030Z; git: bc17a0369a11ecf40bb92c839b9ef0a8ac50ea18; jvm 1.8.0_462-b08
25/11/26 06:53:46 INFO SslContextFactory: x509=X509@348a2f49(1,h=[az-chinanorth2.workers.prod.ns.databricks.com],a=[],w=[]) for Server@16e557d4[provider=null,keyStore=null,trustStore=null]
25/11/26 06:53:46 INFO SslContextFactory: No Cipher Suite matching 'TLS_CHACHA20_POLY1305_SHA256' is supported
25/11/26 06:53:46 WARN config: Weak cipher suite TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA enabled for Server@16e557d4[provider=null,keyStore=null,trustStore=null]
25/11/26 06:53:46 WARN config: Weak cipher suite TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA enabled for Server@16e557d4[provider=null,keyStore=null,trustStore=null]
25/11/26 06:53:46 INFO AbstractConnector: Started ServerConnector@152e530d{SSL, (ssl, http/1.1)}{0.0.0.0:6062}
25/11/26 06:53:46 INFO Server: Started @31120ms
25/11/26 06:53:46 INFO DriverDaemon: Started comm channel server
25/11/26 06:53:46 INFO DriverDaemon: Driver daemon started.
25/11/26 06:53:46 INFO DynamicInfoServiceConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:46 INFO DynamicInfoServiceConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:46 WARN DynamicInfoServiceConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value
25/11/26 06:53:46 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:42657 with 4.4 GiB RAM, BlockManagerId(0, 10.139.64.4, 42657, None)
25/11/26 06:53:47 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
25/11/26 06:53:47 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
25/11/26 06:53:47 INFO DriverCorral: Loading the root classloader
25/11/26 06:53:47 INFO DriverCorral: Starting sql repl ReplId-4b41b-e8c7f-56e3f-c
25/11/26 06:53:47 INFO DriverCorral: Starting sql repl ReplId-31f6b-62b28-53aa3-4
25/11/26 06:53:47 INFO DriverCorral: Starting sql repl ReplId-55108-b279d-02f18-1
25/11/26 06:53:47 INFO SQLDriverWrapper: setupRepl:ReplId-4b41b-e8c7f-56e3f-c: finished to load
25/11/26 06:53:47 INFO SQLDriverWrapper: setupRepl:ReplId-31f6b-62b28-53aa3-4: finished to load
25/11/26 06:53:47 INFO SQLDriverWrapper: setupRepl:ReplId-55108-b279d-02f18-1: finished to load
25/11/26 06:53:47 INFO DriverCorral: Starting sql repl ReplId-75d1b-8bdc3-9850c-7
25/11/26 06:53:47 INFO SQLDriverWrapper: setupRepl:ReplId-75d1b-8bdc3-9850c-7: finished to load
25/11/26 06:53:48 INFO DriverCorral: Starting sql repl ReplId-50361-e4ee1-066a5-3
25/11/26 06:53:48 INFO SQLDriverWrapper: setupRepl:ReplId-50361-e4ee1-066a5-3: finished to load
25/11/26 06:53:48 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
25/11/26 06:53:48 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
25/11/26 06:53:48 INFO DriverCorral: Starting r repl ReplId-3b06b-b12c0-6d682-8
25/11/26 06:53:48 INFO ROutputStreamHandler: Connection succeeded on port 34227
25/11/26 06:53:48 INFO ROutputStreamHandler: Connection succeeded on port 45045
25/11/26 06:53:48 INFO RDriverLocal: 1. RDriverLocal.6024ae8b-61c8-4d07-a746-52f43a012d01: object created with for ReplId-3b06b-b12c0-6d682-8.
25/11/26 06:53:48 INFO RDriverLocal: 2. RDriverLocal.6024ae8b-61c8-4d07-a746-52f43a012d01: initializing ...
25/11/26 06:53:48 INFO RDriverLocal: 3. RDriverLocal.6024ae8b-61c8-4d07-a746-52f43a012d01: started RBackend thread on port 43667
25/11/26 06:53:48 INFO RDriverLocal: 4. RDriverLocal.6024ae8b-61c8-4d07-a746-52f43a012d01: waiting for SparkR to be installed ...
25/11/26 06:53:48 WARN DriverDaemon: ShouldUseAutoscalingInfo exception thrown, not logging stack trace. This is used for control flow and is ok to ignore
25/11/26 06:53:48 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
25/11/26 06:53:48 INFO DynamicTracingConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:48 INFO DynamicTracingConf: Configured feature flag data source LaunchDarkly
25/11/26 06:53:48 WARN DynamicTracingConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value
25/11/26 06:53:50 WARN DriverDaemon: ShouldUseAutoscalingInfo exception thrown, not logging stack trace. This is used for control flow and is ok to ignore
25/11/26 06:53:58 INFO RDriverLocal$: SparkR installation completed.
25/11/26 06:53:58 INFO RDriverLocal: 5. RDriverLocal.6024ae8b-61c8-4d07-a746-52f43a012d01: launching R process ...
25/11/26 06:53:58 INFO RDriverLocal: 6. RDriverLocal.6024ae8b-61c8-4d07-a746-52f43a012d01: cgroup isolation disabled, not placing R process in REPL cgroup.
25/11/26 06:53:58 INFO RDriverLocal: 7. RDriverLocal.6024ae8b-61c8-4d07-a746-52f43a012d01: starting R process on port 1100 (attempt 1) ...
25/11/26 06:53:58 INFO RDriverLocal$: Debugging command for R process builder: SIMBASPARKINI=/etc/simba.sparkodbc.ini R_LIBS=/local_disk0/.ephemeral_nfs/envs/rEnv-cb02e7ba-1b15-4569-a9b9-cbeced7d0da5:/databricks/spark/R/lib:/local_disk0/.ephemeral_nfs/cluster_libraries/r LD_LIBRARY_PATH=/opt/simba/sparkodbc/lib/64/ SPARKR_BACKEND_CONNECTION_TIMEOUT=604800 DB_STREAM_BEACON_STRING_START=DATABRICKS_STREAM_START-ReplId-3b06b-b12c0-6d682-8 DB_STDOUT_STREAM_PORT=34227 SPARKR_BACKEND_AUTH_SECRET=440d99aafdc7c865695075b6b7f25ad5a2b4d7f91d9d07d529a28a057b34d993 DB_STREAM_BEACON_STRING_END=DATABRICKS_STREAM_END-ReplId-3b06b-b12c0-6d682-8 EXISTING_SPARKR_BACKEND_PORT=43667 ODBCINI=/etc/odbc.ini DB_STDERR_STREAM_PORT=45045 /bin/bash /local_disk0/tmp/_startR.sh5110960970988056050resource.r /local_disk0/tmp/_rServeScript.r5498942341103122255resource.r 1100 None
25/11/26 06:53:58 INFO RDriverLocal: 8. RDriverLocal.6024ae8b-61c8-4d07-a746-52f43a012d01: setting up BufferedStreamThread with bufferSize: 1000.
25/11/26 06:53:59 INFO RDriverLocal: 9. RDriverLocal.6024ae8b-61c8-4d07-a746-52f43a012d01: R process started with RServe listening on port 1100.
25/11/26 06:53:59 INFO RDriverLocal: 10. RDriverLocal.6024ae8b-61c8-4d07-a746-52f43a012d01: starting interpreter to talk to R process ...
25/11/26 06:54:00 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
25/11/26 06:54:00 INFO ROutputStreamHandler: Successfully connected to stdout in the RShell.
25/11/26 06:54:00 INFO ROutputStreamHandler: Successfully connected to stderr in the RShell.
25/11/26 06:54:00 INFO RDriverLocal: 11. RDriverLocal.6024ae8b-61c8-4d07-a746-52f43a012d01: R interpreter is connected.
25/11/26 06:54:00 INFO RDriverWrapper: setupRepl:ReplId-3b06b-b12c0-6d682-8: finished to load
25/11/26 06:54:44 INFO DriverCorral: Received SAFEr configs with version 1764127249701
25/11/26 06:54:44 ERROR CommandLineHelper$: Command [REDACTED] failed with exit code 1 out: err:
25/11/26 06:55:44 INFO DriverCorral: Received SAFEr configs with version 1764127249701
25/11/26 06:55:44 ERROR CommandLineHelper$: Command [REDACTED] failed with exit code 1 out: err:
25/11/26 06:56:44 INFO DriverCorral: Received SAFEr configs with version 1764127249701
25/11/26 06:56:44 ERROR CommandLineHelper$: Command [REDACTED] failed with exit code 1 out: err:
25/11/26 06:57:44 INFO DriverCorral: Received SAFEr configs with version 1764127249701
25/11/26 06:57:44 ERROR CommandLineHelper$: Command [REDACTED] failed with exit code 1 out: err:
25/11/26 06:58:00 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
25/11/26 06:58:00 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
25/11/26 06:58:00 INFO DriverCorral: Starting python repl ReplId-34cc6-abfbf-4643b-4
25/11/26 06:58:00 INFO JupyterDriverLocal: Starting gateway server for repl ReplId-34cc6-abfbf-4643b-4
25/11/26 06:58:00 INFO PythonPy4JUtil: Using pinned thread mode in Py4J
25/11/26 06:58:02 INFO VirtualenvCloneHelper: Creating notebook-scoped virtualenv for 0b307e5e-7059-41a1-9ba8-90a1694acde5
25/11/26 06:58:02 INFO Utils: resolved command to be run: List(virtualenv, /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b307e5e-7059-41a1-9ba8-90a1694acde5, -p, /local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/python, --no-download, --no-setuptools, --no-wheel)
25/11/26 06:58:02 INFO PythonEnvCloneHelper$: Created python virtualenv: /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b307e5e-7059-41a1-9ba8-90a1694acde5
25/11/26 06:58:02 INFO Utils: resolved command to be run: List(/local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/python, -c, import sys; dirs=[p for p in sys.path if 'package' in p]; print(' '.join(dirs)))
25/11/26 06:58:03 INFO Utils: resolved command to be run: List(/local_disk0/.ephemeral_nfs/envs/pythonEnv-0b307e5e-7059-41a1-9ba8-90a1694acde5/bin/python, -c, from distutils.sysconfig import get_python_lib; print(get_python_lib()))
25/11/26 06:58:03 INFO PythonEnvCloneHelper$: Created sites.pth at /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b307e5e-7059-41a1-9ba8-90a1694acde5/lib/python3.9/site-packages/sites.pth
25/11/26 06:58:03 INFO NotebookScopedPythonEnvManager: Time spent to start virtualenv /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b307e5e-7059-41a1-9ba8-90a1694acde5 is 668(ms)
25/11/26 06:58:03 INFO NotebookScopedPythonEnvManager: Registered /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b307e5e-7059-41a1-9ba8-90a1694acde5/lib/python3.9/site-packages with the WatchService sun.nio.fs.LinuxWatchService$LinuxWatchKey@6e93f5ee
25/11/26 06:58:03 INFO IpykernelUtils$: Python process builder: [bash, /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b307e5e-7059-41a1-9ba8-90a1694acde5/python_start_0b307e5e-7059-41a1-9ba8-90a1694acde5.sh, /databricks/spark/python/pyspark/wrapped_python.py, root, /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b307e5e-7059-41a1-9ba8-90a1694acde5/bin/python, /databricks/python_shell/scripts/db_ipykernel_launcher.py, -f, /databricks/kernel-connections/9ae50f1cfe69d5fc801f58228130ddc503d4a753ce44eee168c0b079ced9d083.json]
25/11/26 06:58:03 INFO IpykernelUtils$: Cgroup isolation disabled, not placing python process in repl cgroup
25/11/26 06:58:07 INFO PythonDriverWrapper: setupRepl:ReplId-34cc6-abfbf-4643b-4: finished to load
25/11/26 06:58:07 INFO DriverConf: Configured feature flag data source LaunchDarkly
25/11/26 06:58:07 INFO DriverConf: Configured feature flag data source LaunchDarkly
25/11/26 06:58:07 WARN DriverConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value
25/11/26 06:58:07 INFO ProgressReporter$: Reporting progress for running commands: 3804533157001577396_5652356821157556155_run-400333104312677-precondition
25/11/26 06:58:07 INFO ProgressReporter$: Added result fetcher for 3804533157001577396_5652356821157556155_run-400333104312677-precondition
25/11/26 06:58:08 INFO ProgressReporter$: Removed result fetcher for 3804533157001577396_5652356821157556155_run-400333104312677-precondition
25/11/26 06:58:11 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
25/11/26 06:58:11 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
25/11/26 06:58:11 INFO DriverCorral: Starting python repl ReplId-1ce26-dea4c-8a10e-1
25/11/26 06:58:11 INFO JupyterDriverLocal: Starting gateway server for repl ReplId-1ce26-dea4c-8a10e-1
25/11/26 06:58:11 INFO PythonPy4JUtil: Using pinned thread mode in Py4J
25/11/26 06:58:11 INFO VirtualenvCloneHelper: Creating notebook-scoped virtualenv for 2d6622a0-0617-4f8a-8064-5b99ca270e58
25/11/26 06:58:11 INFO Utils: resolved command to be run: List(virtualenv, /local_disk0/.ephemeral_nfs/envs/pythonEnv-2d6622a0-0617-4f8a-8064-5b99ca270e58, -p, /local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/python, --no-download, --no-setuptools, --no-wheel)
25/11/26 06:58:12 INFO PythonEnvCloneHelper$: Created python virtualenv: /local_disk0/.ephemeral_nfs/envs/pythonEnv-2d6622a0-0617-4f8a-8064-5b99ca270e58
25/11/26 06:58:12 INFO Utils: resolved command to be run: List(/local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/python, -c, import sys; dirs=[p for p in sys.path if 'package' in p]; print(' '.join(dirs)))
25/11/26 06:58:12 INFO Utils: resolved command to be run: List(/local_disk0/.ephemeral_nfs/envs/pythonEnv-2d6622a0-0617-4f8a-8064-5b99ca270e58/bin/python, -c, from distutils.sysconfig import get_python_lib; print(get_python_lib()))
25/11/26 06:58:12 INFO PythonEnvCloneHelper$: Created sites.pth at /local_disk0/.ephemeral_nfs/envs/pythonEnv-2d6622a0-0617-4f8a-8064-5b99ca270e58/lib/python3.9/site-packages/sites.pth
25/11/26 06:58:12 INFO NotebookScopedPythonEnvManager: Time spent to start virtualenv /local_disk0/.ephemeral_nfs/envs/pythonEnv-2d6622a0-0617-4f8a-8064-5b99ca270e58 is 279(ms)
25/11/26 06:58:12 INFO NotebookScopedPythonEnvManager: Registered /local_disk0/.ephemeral_nfs/envs/pythonEnv-2d6622a0-0617-4f8a-8064-5b99ca270e58/lib/python3.9/site-packages with the WatchService sun.nio.fs.LinuxWatchService$LinuxWatchKey@9b309e7
25/11/26 06:58:12 INFO IpykernelUtils$: Python process builder: [bash, /local_disk0/.ephemeral_nfs/envs/pythonEnv-2d6622a0-0617-4f8a-8064-5b99ca270e58/python_start_2d6622a0-0617-4f8a-8064-5b99ca270e58.sh, /databricks/spark/python/pyspark/wrapped_python.py, root, /local_disk0/.ephemeral_nfs/envs/pythonEnv-2d6622a0-0617-4f8a-8064-5b99ca270e58/bin/python, /databricks/python_shell/scripts/db_ipykernel_launcher.py, -f, /databricks/kernel-connections/36cc57b583d30ad54568dc8045b291ecd2014a843a4dee0750dce5ad4d6f4ccc.json]
25/11/26 06:58:12 INFO IpykernelUtils$: Cgroup isolation disabled, not placing python process in repl cgroup
25/11/26 06:58:13 INFO PythonDriverWrapper: setupRepl:ReplId-1ce26-dea4c-8a10e-1: finished to load
25/11/26 06:58:13 INFO ProgressReporter$: Added result fetcher for 2081346830872482017_5482021896786453453_b8107ed1-7539-4b59-88bc-5a916359ed8e
25/11/26 06:58:13 INFO ProgressReporter$: Removed result fetcher for 2081346830872482017_5482021896786453453_b8107ed1-7539-4b59-88bc-5a916359ed8e
25/11/26 06:58:13 INFO ProgressReporter$: Added result fetcher for 2081346830872482017_7571052252010323293_job-929358892198333-run-400333104312677-action-5022948843659687
25/11/26 06:58:14 INFO ProgressReporter$: Reporting partial results for running commands: 2081346830872482017_7571052252010323293_job-929358892198333-run-400333104312677-action-5022948843659687
25/11/26 06:58:15 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.007926 ms.
25/11/26 06:58:15 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.051941 ms.
25/11/26 06:58:17 INFO DriverConf: Configured feature flag data source LaunchDarkly
25/11/26 06:58:17 INFO DriverConf: Configured feature flag data source LaunchDarkly
25/11/26 06:58:17 WARN DriverConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value
25/11/26 06:58:17 ERROR AbfsClient: HttpRequest: 403,err=,appendpos=,cid=1125-113639-ndjs76hq------:13ee7ca0-9ec0-4b07-8729-0f8401be92d2:97de1a12-b907-47e4-83d9-7075be68f6d0:::GF:0,rid=e3b05305-601f-004b-40a2-5ebf80000000,connMs=0,sendMs=0,recvMs=17,sent=0,recv=0,method=HEAD,https://tarhonemetastore.dfs.core.chinacloudapi.cn/uctarhone/_encryption_meta/manifest.json?upn=false&action=getStatus&timeout=90&st=2025-11-26T06:48:17Z&sv=2020-02-10&ske=2025-11-26T08:48:17Z&sig=XXXXX&sktid=073e223d-89a0-4197-b5c0-096aced1ef02&se=2025-11-26T07:58:17Z&sdd=4&skoid=b6eba1ca-db96-49f9XXXXXXXXXXXXXXXXXX&spr=https&sks=b&skt=2025-11-26T06:48:17Z&sp=racwdxlm&skv=2025-01-05&sr=d
25/11/26 06:58:17 WARN FallbackEncryptionContextProvider: Accessing the manifest file failed with 403.
25/11/26 06:58:17 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1 with credential = CredentialScopeADLSTokenProvider with jvmId = 574
25/11/26 06:58:17 ERROR AbfsClient: HttpRequest: 409,err=PathAlreadyExists,appendpos=,cid=1125-113639-ndjs76hq------:d3fff097-5ac2-4456-bcc6-abed42772899:97de1a12-b907-47e4-83d9-7075be68f6d0:::MK:0,rid=e3b05306-601f-004b-41a2-5ebf80000000,connMs=1,sendMs=1,recvMs=76,sent=0,recv=168,method=PUT,https://tarhonemetastore.dfs.core.chinacloudapi.cn/uctarhone/tarhoneroot1/bronze/test/vwtable1?resource=directory&timeout=90&st=2025-11-26T06:48:17Z&sv=2020-02-10&ske=2025-11-26T08:48:17Z&sig=XXXXX&sktid=073e223d-89a0-4197-b5c0-096aced1ef02&se=2025-11-26T07:58:17Z&sdd=4&skoid=b6eba1ca-db96-49f9XXXXXXXXXXXXXXXXXX&spr=https&sks=b&skt=2025-11-26T06:48:17Z&sp=racwdxlm&skv=2025-01-05&sr=d
25/11/26 06:58:17 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1 with credential = CredentialScopeADLSTokenProvider with jvmId = 574
25/11/26 06:58:17 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.014406 ms.
25/11/26 06:58:17 INFO ClusterLoadMonitor: Added query with execution ID:0. Current active queries:1
25/11/26 06:58:17 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
25/11/26 06:58:19 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 0.0, New Ema: 1.0 
25/11/26 06:58:20 INFO DeltaLog: Loading version 10 starting from checkpoint version 10.
25/11/26 06:58:21 INFO SnapshotEdge: [tableId=d1aef15c-c2c5-4565-8952-581b79bd6dc4] Created snapshot SnapshotEdge(path=abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log, version=10, metadata=Metadata(62bed5a3-0999-4586-90cd-7b6af6e36be7,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{"scale":0}},{"name":"name","type":"string","nullable":true,"metadata":{"scale":0}},{"name":"age","type":"integer","nullable":true,"metadata":{"scale":0}},{"name":"grade","type":"decimal(4,2)","nullable":true,"metadata":{"scale":2}},{"name":"birthday","type":"date","nullable":true,"metadata":{"scale":0}},{"name":"logintime","type":"timestamp","nullable":true,"metadata":{"scale":6}}]},List(),Map(),Some(1764131452056)), logSegment=LogSegment(abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log,10,WrappedArray(),WrappedArray(VersionedFileStatus{VersionedFileStatus{path=abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/00000000000000000010.checkpoint.parquet; isDirectory=false; length=25741; replication=1; blocksize=268435456; modification_time=1764139674000; access_time=0; owner=b6eba1ca-db96-49f9-b19f-f309bce4121d; group=$superuser; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}; version='0x8DE2CB7B9F4F209'}),Some(10),1764139670000), checksumOpt=Some(VersionChecksum(Some(c2305be3-3b61-4213-9c99-98378ff5274b),3241375616,8,None,None,1,1,Some(List()),None,Metadata(62bed5a3-0999-4586-90cd-7b6af6e36be7,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{"scale":0}},{"name":"name","type":"string","nullable":true,"metadata":{"scale":0}},{"name":"age","type":"integer","nullable":true,"metadata":{"scale":0}},{"name":"grade","type":"decimal(4,2)","nullable":true,"metadata":{"scale":2}},{"name":"birthday","type":"date","nullable":true,"metadata":{"scale":0}},{"name":"logintime","type":"timestamp","nullable":true,"metadata":{"scale":6}}]},List(),Map(),Some(1764131452056)),Protocol(1,2),Some(FileSizeHistogram(Vector(0, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576, 2097152, 4194304, 8388608, 12582912, 16777216, 20971520, 25165824, 29360128, 33554432, 37748736, 41943040, 50331648, 58720256, 67108864, 75497472, 83886080, 92274688, 100663296, 109051904, 117440512, 125829120, 130023424, 134217728, 138412032, 142606336, 146800640, 150994944, 167772160, 184549376, 201326592, 218103808, 234881024, 251658240, 268435456, 285212672, 301989888, 318767104, 335544320, 352321536, 369098752, 385875968, 402653184, 419430400, 436207616, 452984832, 469762048, 486539264, 503316480, 520093696, 536870912, 553648128, 570425344, 587202560, 603979776, 671088640, 738197504, 805306368, 872415232, 939524096, 1006632960, 1073741824, 1140850688, 1207959552, 1275068416, 1342177280, 1409286144, 1476395008, 1610612736, 1744830464, 1879048192, 2013265920, 2147483648, 2415919104, 2684354560, 2952790016, 3221225472, 3489660928, 3758096384, 4026531840, 4294967296, 8589934592, 17179869184, 34359738368, 68719476736, 137438953472, 274877906944),[J@412e351a,[J@152bd5f0)),None,Some(List(AddFile(part-00000-5e1c2199-b033-4e49-8d2e-d75a95e4871f-c000.snappy.parquet,Map(),405171952,1764135958000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764135958000000, MIN_INSERTION_TIME -> 1764135958000000, MAX_INSERTION_TIME -> 1764135958000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-884d449d-510d-499b-86b0-788d47e58357-c000.snappy.parquet,Map(),405171952,1764136774000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764136774000000, MIN_INSERTION_TIME -> 1764136774000000, MAX_INSERTION_TIME -> 1764136774000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-c07e2cc6-f2ac-44bd-a972-b2db6f803a6e-c000.snappy.parquet,Map(),405171952,1764135057000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764135057000000, MIN_INSERTION_TIME -> 1764135057000000, MAX_INSERTION_TIME -> 1764135057000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-e5f2304d-bad7-491d-ad4c-0e85916c37a6-c000.snappy.parquet,Map(),405171952,1764137105000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764137105000000, MIN_INSERTION_TIME -> 1764137105000000, MAX_INSERTION_TIME -> 1764137105000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-93ed7c89-5969-4fcc-bc14-6c931e8bceaa-c000.snappy.parquet,Map(),405171952,1764139669000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764139669000000, MIN_INSERTION_TIME -> 1764139669000000, MAX_INSERTION_TIME -> 1764139669000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-c15fd30b-8147-4e96-a419-0a7fe6f8953a-c000.snappy.parquet,Map(),405171952,1764136406000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764136406000000, MIN_INSERTION_TIME -> 1764136406000000, MAX_INSERTION_TIME -> 1764136406000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-4a2e5635-bd9b-4892-9e39-c53414c85b07-c000.snappy.parquet,Map(),405171952,1764135429000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764135429000000, MIN_INSERTION_TIME -> 1764135429000000, MAX_INSERTION_TIME -> 1764135429000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-9bcc8d32-27ed-44cf-b137-a1cee0b2926e-c000.snappy.parquet,Map(),405171952,1764137553000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764137553000000, MIN_INSERTION_TIME -> 1764137553000000, MAX_INSERTION_TIME -> 1764137553000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None))))))
25/11/26 06:58:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 436.5 KiB, free 4.4 GiB)
25/11/26 06:58:22 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 156.7 KiB, free 4.4 GiB)
25/11/26 06:58:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 4.4 GiB)
25/11/26 06:58:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.139.64.5:39807 (size: 18.5 KiB, free: 4.4 GiB)
25/11/26 06:58:22 INFO SparkContext: Created broadcast 1 from writeExternal at ObjectOutputStream.java:1459
25/11/26 06:58:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.8 KiB, free 4.4 GiB)
25/11/26 06:58:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.139.64.5:39807 (size: 15.8 KiB, free: 4.4 GiB)
25/11/26 06:58:22 INFO SparkContext: Created broadcast 0 from broadcast at DeltaLog.scala:718
25/11/26 06:58:22 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(Parquet, numFilesInSegment: 1, totalFileSize: 25741)
25/11/26 06:58:22 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:58:24 INFO CodeGenerator: Code generated in 653.619049 ms
25/11/26 06:58:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 32.7 KiB, free 4.4 GiB)
25/11/26 06:58:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.5 KiB, free 4.4 GiB)
25/11/26 06:58:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.139.64.5:39807 (size: 14.5 KiB, free: 4.4 GiB)
25/11/26 06:58:24 INFO SparkContext: Created broadcast 2 from $anonfun$executePhase$2 at LexicalThreadLocal.scala:63
25/11/26 06:58:24 INFO CodeGenerator: Code generated in 139.585077 ms
25/11/26 06:58:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 40.2 KiB, free 4.4 GiB)
25/11/26 06:58:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 4.4 GiB)
25/11/26 06:58:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.139.64.5:39807 (size: 12.5 KiB, free: 4.4 GiB)
25/11/26 06:58:24 INFO SparkContext: Created broadcast 3 from $anonfun$executePhase$2 at LexicalThreadLocal.scala:63
25/11/26 06:58:24 INFO FileSourceStrategy: Pushed Filters: 
25/11/26 06:58:24 INFO FileSourceStrategy: Post-Scan Filters: 
25/11/26 06:58:25 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
25/11/26 06:58:25 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:58:25 INFO CodeGenerator: Code generated in 140.989325 ms
25/11/26 06:58:25 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 537.7 KiB, free 4.4 GiB)
25/11/26 06:58:25 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 4.4 GiB)
25/11/26 06:58:25 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.139.64.5:39807 (size: 19.5 KiB, free: 4.4 GiB)
25/11/26 06:58:25 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63
25/11/26 06:58:26 INFO FileSourceScanExec: Planning scan with bin packing, max split size: 134217728 bytes, max partition size: 4194304, open cost is considered as scanning 4194304 bytes.
25/11/26 06:58:26 INFO RocksDBLoader: RocksDB library loading thread started
25/11/26 06:58:26 INFO RocksDBLoader: RocksDB library loading thread finished successfully
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] RocksDB version: 6.28.2

25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Compile date 2022-02-04 22:23:17
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] DB SUMMARY

25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] DB Session ID:  G9YTGBZZ6DL6Y3NXR7SL

25/11/26 06:58:26 ERROR RocksDBExternalMap: [Native-3] Error when reading /local_disk0/dbio_cache_root_574-9ed7a902-0679-42d3-a994-133a05da1172/locality_assignments-7734b17f-0737-4f2d-925b-c2ad6f0ce66d/13230108913530 dir

25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] SST files in /local_disk0/dbio_cache_root_574-9ed7a902-0679-42d3-a994-133a05da1172/locality_assignments-7734b17f-0737-4f2d-925b-c2ad6f0ce66d/13230108913530 dir, Total Num: 0, files: 

25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Write Ahead Log file in /local_disk0/dbio_cache_root_574-9ed7a902-0679-42d3-a994-133a05da1172/locality_assignments-7734b17f-0737-4f2d-925b-c2ad6f0ce66d/13230108913530: 

25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                         Options.error_if_exists: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                       Options.create_if_missing: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                         Options.paranoid_checks: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.flush_verify_memtable_count: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                               Options.track_and_verify_wals_in_manifest: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                                     Options.env: 0x7f6fce304780
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                                      Options.fs: PosixFileSystem
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                                Options.info_log: 0x7f703c138a48
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                Options.max_file_opening_threads: 16
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                              Options.statistics: (nil)
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                               Options.use_fsync: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                       Options.max_log_file_size: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                  Options.max_manifest_file_size: 1073741824
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                   Options.log_file_time_to_roll: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                       Options.keep_log_file_num: 1000
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                    Options.recycle_log_file_num: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                         Options.allow_fallocate: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                        Options.allow_mmap_reads: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                       Options.allow_mmap_writes: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                        Options.use_direct_reads: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                        Options.use_direct_io_for_flush_and_compaction: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]          Options.create_missing_column_families: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                              Options.db_log_dir: 
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                                 Options.wal_dir: 
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                Options.table_cache_numshardbits: 6
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                         Options.WAL_ttl_seconds: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                       Options.WAL_size_limit_MB: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                        Options.max_write_batch_group_size_bytes: 1048576
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.manifest_preallocation_size: 4194304
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                     Options.is_fd_close_on_exec: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                   Options.advise_random_on_open: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                   Options.experimental_mempurge_threshold: 0.000000
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                    Options.db_write_buffer_size: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                    Options.write_buffer_manager: 0x7f703c1278a0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]         Options.access_hint_on_compaction_start: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]  Options.new_table_reader_for_compaction_inputs: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]           Options.random_access_max_buffer_size: 1048576
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                      Options.use_adaptive_mutex: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                            Options.rate_limiter: (nil)
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]     Options.sst_file_manager.rate_bytes_per_sec: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                       Options.wal_recovery_mode: 2
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                  Options.enable_thread_tracking: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                  Options.enable_pipelined_write: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                  Options.unordered_write: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]         Options.allow_concurrent_memtable_write: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]      Options.enable_write_thread_adaptive_yield: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.write_thread_max_yield_usec: 100
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]            Options.write_thread_slow_yield_usec: 3
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                               Options.row_cache: None
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                              Options.wal_filter: None
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.avoid_flush_during_recovery: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.allow_ingest_behind: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.preserve_deletes: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.two_write_queues: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.manual_wal_flush: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.atomic_flush: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.avoid_unnecessary_blocking_io: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                 Options.persist_stats_to_disk: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                 Options.write_dbid_to_manifest: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                 Options.log_readahead_size: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                 Options.file_checksum_gen_factory: Unknown
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                 Options.best_efforts_recovery: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                Options.max_bgerror_resume_count: 2147483647
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]            Options.bgerror_resume_retry_interval: 1000000
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.allow_data_in_errors: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.db_host_id: __hostname__
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.max_background_jobs: 2
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.max_background_compactions: -1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.max_subcompactions: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.avoid_flush_during_shutdown: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]           Options.writable_file_max_buffer_size: 1048576
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.delayed_write_rate : 16777216
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.max_total_wal_size: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.delete_obsolete_files_period_micros: 21600000000
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                   Options.stats_dump_period_sec: 600
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                 Options.stats_persist_period_sec: 600
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                 Options.stats_history_buffer_size: 1048576
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                          Options.max_open_files: -1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                          Options.bytes_per_sync: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                      Options.wal_bytes_per_sync: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                   Options.strict_bytes_per_sync: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]       Options.compaction_readahead_size: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                  Options.max_background_flushes: -1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Compression algorithms supported:
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] 	kZSTDNotFinalCompression supported: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] 	kZSTD supported: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] 	kXpressCompression supported: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] 	kLZ4HCCompression supported: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] 	kLZ4Compression supported: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] 	kBZip2Compression supported: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] 	kZlibCompression supported: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] 	kSnappyCompression supported: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Fast CRC32 supported: Not supported on x86
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] [/db_impl/db_impl_open.cc:307] Creating manifest 1 

25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] [/version_set.cc:4847] Recovering from manifest file: /local_disk0/dbio_cache_root_574-9ed7a902-0679-42d3-a994-133a05da1172/locality_assignments-7734b17f-0737-4f2d-925b-c2ad6f0ce66d/13230108913530/MANIFEST-000001

25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] [/column_family.cc:607] --------------- Options for column family [default]:

25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]               Options.comparator: leveldb.BytewiseComparator
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]           Options.merge_operator: None
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]        Options.compaction_filter: None
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]        Options.compaction_filter_factory: None
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]  Options.sst_partitioner_factory: None
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]         Options.memtable_factory: SkipListFactory
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]            Options.table_factory: BlockBasedTable
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f703c13a140)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  hash_index_allow_collision: 1
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f703c138d30
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0

25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]        Options.write_buffer_size: 67108864
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]  Options.max_write_buffer_number: 2
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]          Options.compression: Snappy
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                  Options.bottommost_compression: Disabled
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]       Options.prefix_extractor: nullptr
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]   Options.memtable_insert_with_hint_prefix_extractor: nullptr
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.num_levels: 7
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]        Options.min_write_buffer_number_to_merge: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]     Options.max_write_buffer_number_to_maintain: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]     Options.max_write_buffer_size_to_maintain: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]            Options.bottommost_compression_opts.window_bits: -14
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                  Options.bottommost_compression_opts.level: 32767
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]               Options.bottommost_compression_opts.strategy: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]         Options.bottommost_compression_opts.max_dict_bytes: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]         Options.bottommost_compression_opts.parallel_threads: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                  Options.bottommost_compression_opts.enabled: false
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]            Options.compression_opts.window_bits: -14
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                  Options.compression_opts.level: 32767
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]               Options.compression_opts.strategy: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]         Options.compression_opts.max_dict_bytes: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]         Options.compression_opts.zstd_max_train_bytes: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]         Options.compression_opts.parallel_threads: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                  Options.compression_opts.enabled: false
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]         Options.compression_opts.max_dict_buffer_bytes: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]      Options.level0_file_num_compaction_trigger: 4
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]          Options.level0_slowdown_writes_trigger: 20
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]              Options.level0_stop_writes_trigger: 36
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                   Options.target_file_size_base: 67108864
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]             Options.target_file_size_multiplier: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                Options.max_bytes_for_level_base: 268435456
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.level_compaction_dynamic_level_bytes: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]          Options.max_bytes_for_level_multiplier: 10.000000
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.max_bytes_for_level_multiplier_addtl[0]: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.max_bytes_for_level_multiplier_addtl[1]: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.max_bytes_for_level_multiplier_addtl[2]: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.max_bytes_for_level_multiplier_addtl[3]: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.max_bytes_for_level_multiplier_addtl[4]: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.max_bytes_for_level_multiplier_addtl[5]: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.max_bytes_for_level_multiplier_addtl[6]: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]       Options.max_sequential_skip_in_iterations: 8
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                    Options.max_compaction_bytes: 1677721600
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                        Options.arena_block_size: 1048576
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]   Options.soft_pending_compaction_bytes_limit: 68719476736
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]   Options.hard_pending_compaction_bytes_limit: 274877906944
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]       Options.rate_limit_delay_max_milliseconds: 100
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                Options.disable_auto_compactions: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                        Options.compaction_style: kCompactionStyleLevel
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                          Options.compaction_pri: kMinOverlappingRatio
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.compaction_options_universal.size_ratio: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.compaction_options_universal.min_merge_width: 2
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.compaction_options_universal.max_merge_width: 4294967295
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.compaction_options_universal.max_size_amplification_percent: 200
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.compaction_options_universal.compression_size_percent: -1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.compaction_options_fifo.max_table_files_size: 1073741824
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.compaction_options_fifo.allow_compaction: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                   Options.table_properties_collectors: 
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                   Options.inplace_update_support: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                 Options.inplace_update_num_locks: 10000
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]               Options.memtable_prefix_bloom_size_ratio: 0.000000
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]               Options.memtable_whole_key_filtering: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]   Options.memtable_huge_page_size: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                           Options.bloom_locality: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                    Options.max_successive_merges: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                Options.optimize_filters_for_hits: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                Options.paranoid_file_checks: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                Options.force_consistency_checks: 1
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                Options.report_bg_io_stats: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                               Options.ttl: 2592000
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]          Options.periodic_compaction_seconds: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                       Options.enable_blob_files: false
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                           Options.min_blob_size: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                          Options.blob_file_size: 268435456
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]                   Options.blob_compression_type: NoCompression
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]          Options.enable_blob_garbage_collection: false
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]      Options.blob_garbage_collection_age_cutoff: 0.250000
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] Options.blob_garbage_collection_force_threshold: 1.000000
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1]          Options.blob_compaction_readahead_size: 0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] [/version_set.cc:4895] Recovered from manifest file:/local_disk0/dbio_cache_root_574-9ed7a902-0679-42d3-a994-133a05da1172/locality_assignments-7734b17f-0737-4f2d-925b-c2ad6f0ce66d/13230108913530/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0,min_log_number_to_keep is 0

25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] [/version_set.cc:4904] Column family [default] (ID 0), log number is 0

25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] [/version_set.cc:4385] Creating manifest 4

25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] [/db_impl/db_impl_open.cc:1793] SstFileManager instance 0x7f703c04e9c0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] DB pointer 0x7f703c127de0
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] [/db_impl/db_impl.cc:1005] ------- DUMPING STATS -------
25/11/26 06:58:26 INFO RocksDBExternalMap: [Native-1] [/db_impl/db_impl.cc:1006] 
** DB Stats **
Uptime(secs): 0.0 total, 0.0 interval
Cumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s
Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Cumulative stall: 00:00:0.000 H:M:S, 0.0 percent
Interval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s
Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Interval stall: 00:00:0.000 H:M:S, 0.0 percent

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Sum      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Blob file count: 0, total size: 0.0 GB

Uptime(secs): 0.0 total, 0.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count
Block cache LRUCache@0x7f703c138d30#574 capacity: 8.00 MB collections: 1 last_copies: 0 last_secs: 4e-05 secs_since: 0
Block cache entry stats(count,size,portion): Misc(1,0.00 KB,0%)

** File Read Latency Histogram By Level [default] **

25/11/26 06:58:26 INFO DAGScheduler: Registering RDD 3 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) as input to shuffle 0
25/11/26 06:58:26 INFO DAGScheduler: Got map stage job 0 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) with 1 output partitions
25/11/26 06:58:26 INFO DAGScheduler: Final stage: ShuffleMapStage 0 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63)
25/11/26 06:58:26 INFO DAGScheduler: Parents of final stage: List()
25/11/26 06:58:26 INFO DAGScheduler: Missing parents: List()
25/11/26 06:58:26 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63), which has no missing parents
25/11/26 06:58:26 INFO DAGScheduler: Jars for session None: Map()
25/11/26 06:58:26 INFO DAGScheduler: Files for session None: Map()
25/11/26 06:58:26 INFO DAGScheduler: Archives for session None: Map()
25/11/26 06:58:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) (first 15 tasks are for partitions Vector(0))
25/11/26 06:58:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/11/26 06:58:26 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 2081346830872482017, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 2081346830872482017. Created 2081346830872482017 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
25/11/26 06:58:26 INFO FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool 2081346830872482017
25/11/26 06:58:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.139.64.4, executor 0, partition 0, NODE_LOCAL, taskResourceAssignments Map())
25/11/26 06:58:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 292.7 KiB, free 4.4 GiB)
25/11/26 06:58:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 85.4 KiB, free 4.4 GiB)
25/11/26 06:58:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.139.64.5:39807 (size: 85.4 KiB, free: 4.4 GiB)
25/11/26 06:58:26 INFO SparkContext: Created broadcast 5 from broadcast at TaskSetManager.scala:638
25/11/26 06:58:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.139.64.4:42657 (size: 85.4 KiB, free: 4.4 GiB)
25/11/26 06:58:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.139.64.4:42657 (size: 18.5 KiB, free: 4.4 GiB)
25/11/26 06:58:28 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:58:28 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.139.64.4:42657 (size: 19.5 KiB, free: 4.4 GiB)
25/11/26 06:58:31 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-chinanorth3-prod-metastore-0.mysql.database.chinacloudapi.cn:3306/organization2559323315997869?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
25/11/26 06:58:31 INFO HikariDataSource: metastore-monitor - Starting...
25/11/26 06:58:31 INFO HikariDataSource: metastore-monitor - Start completed.
25/11/26 06:58:31 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
25/11/26 06:58:31 INFO HikariDataSource: metastore-monitor - Shutdown completed.
25/11/26 06:58:31 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 231 milliseconds)
25/11/26 06:58:31 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-chinanorth3-prod-metastore-0.mysql.database.chinacloudapi.cn:9207/organization2559323315997869?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
25/11/26 06:58:31 INFO HikariDataSource: metastore-monitor - Starting...
25/11/26 06:58:31 INFO HikariDataSource: metastore-monitor - Start completed.
25/11/26 06:58:31 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
25/11/26 06:58:31 INFO HikariDataSource: metastore-monitor - Shutdown completed.
25/11/26 06:58:31 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 225 milliseconds)
25/11/26 06:58:31 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:58:34 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:58:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 10229 ms on 10.139.64.4 (executor 0) (1/1)
25/11/26 06:58:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 2081346830872482017
25/11/26 06:58:37 INFO DAGScheduler: ShuffleMapStage 0 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) finished in 10.517 s
25/11/26 06:58:37 INFO DAGScheduler: looking for newly runnable stages
25/11/26 06:58:37 INFO DAGScheduler: running: Set()
25/11/26 06:58:37 INFO DAGScheduler: waiting: Set()
25/11/26 06:58:37 INFO DAGScheduler: failed: Set()
25/11/26 06:58:37 INFO CodeGenerator: Code generated in 57.133446 ms
25/11/26 06:58:37 INFO SQLAppStatusListener: Recording cache-related metrics in usage logs:
ioCacheWorkersDiskUsage={"10.139.64.4":{"diskUsage":4007,"lifetime":291736}}, ioCacheNumScanTasks={"numLocalScanTasks":1,"numNonLocalScanTasks":0}, ioCacheDiskUsageLimit=78696390656
25/11/26 06:58:37 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:58:38 INFO CodeGenerator: Code generated in 55.650998 ms
25/11/26 06:58:38 INFO DAGScheduler: Registering RDD 13 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) as input to shuffle 1
25/11/26 06:58:38 INFO DAGScheduler: Got map stage job 1 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) with 1 output partitions
25/11/26 06:58:38 INFO DAGScheduler: Final stage: ShuffleMapStage 2 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63)
25/11/26 06:58:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
25/11/26 06:58:38 INFO DAGScheduler: Missing parents: List()
25/11/26 06:58:38 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63), which has no missing parents
25/11/26 06:58:38 INFO QueryProfileListener: Query profile sent to logger, seq number: 0, app id: app-20251126065334-0000
25/11/26 06:58:38 INFO DAGScheduler: Jars for session None: Map()
25/11/26 06:58:38 INFO DAGScheduler: Files for session None: Map()
25/11/26 06:58:38 INFO DAGScheduler: Archives for session None: Map()
25/11/26 06:58:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) (first 15 tasks are for partitions Vector(0))
25/11/26 06:58:38 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
25/11/26 06:58:38 INFO FairSchedulableBuilder: Added task set TaskSet_2.0 tasks to pool 2081346830872482017
25/11/26 06:58:38 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (10.139.64.4, executor 0, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
25/11/26 06:58:38 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 443.7 KiB, free 4.4 GiB)
25/11/26 06:58:38 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 127.2 KiB, free 4.4 GiB)
25/11/26 06:58:38 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.139.64.5:39807 (size: 127.2 KiB, free: 4.4 GiB)
25/11/26 06:58:38 INFO SparkContext: Created broadcast 6 from broadcast at TaskSetManager.scala:638
25/11/26 06:58:38 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.139.64.4:42657 (size: 127.2 KiB, free: 4.4 GiB)
25/11/26 06:58:38 INFO AsyncEventQueue: Process of event SparkListenerQueryProfileParamsReady(executionId=1, ...) by listener QueryProfileListener took 1.335036623s.
25/11/26 06:58:39 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.139.64.4:60904
25/11/26 06:58:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.139.64.4:42657 (size: 12.5 KiB, free: 4.4 GiB)
25/11/26 06:58:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.139.64.4:42657 (size: 14.5 KiB, free: 4.4 GiB)
25/11/26 06:58:39 INFO BlockManagerInfo: Added rdd_10_0 in memory on 10.139.64.4:42657 (size: 1804.0 B, free: 4.4 GiB)
25/11/26 06:58:40 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 1764 ms on 10.139.64.4 (executor 0) (1/1)
25/11/26 06:58:40 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 2081346830872482017
25/11/26 06:58:40 INFO DAGScheduler: ShuffleMapStage 2 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) finished in 2.136 s
25/11/26 06:58:40 INFO DAGScheduler: looking for newly runnable stages
25/11/26 06:58:40 INFO DAGScheduler: running: Set()
25/11/26 06:58:40 INFO DAGScheduler: waiting: Set()
25/11/26 06:58:40 INFO DAGScheduler: failed: Set()
25/11/26 06:58:40 INFO SparkContext: Starting job: first at Snapshot.scala:271
25/11/26 06:58:40 INFO DAGScheduler: Got job 2 (first at Snapshot.scala:271) with 1 output partitions
25/11/26 06:58:40 INFO DAGScheduler: Final stage: ResultStage 5 (first at Snapshot.scala:271)
25/11/26 06:58:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
25/11/26 06:58:40 INFO DAGScheduler: Missing parents: List()
25/11/26 06:58:40 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[15] at first at Snapshot.scala:271), which has no missing parents
25/11/26 06:58:40 INFO DAGScheduler: Jars for session None: Map()
25/11/26 06:58:40 INFO DAGScheduler: Files for session None: Map()
25/11/26 06:58:40 INFO DAGScheduler: Archives for session None: Map()
25/11/26 06:58:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[15] at first at Snapshot.scala:271) (first 15 tasks are for partitions Vector(0))
25/11/26 06:58:40 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
25/11/26 06:58:40 INFO FairSchedulableBuilder: Added task set TaskSet_5.0 tasks to pool 2081346830872482017
25/11/26 06:58:40 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 2) (10.139.64.4, executor 0, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
25/11/26 06:58:40 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 388.6 KiB, free 4.4 GiB)
25/11/26 06:58:40 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 116.9 KiB, free 4.4 GiB)
25/11/26 06:58:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.139.64.5:39807 (size: 116.9 KiB, free: 4.4 GiB)
25/11/26 06:58:40 INFO SparkContext: Created broadcast 7 from broadcast at TaskSetManager.scala:638
25/11/26 06:58:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.139.64.4:42657 (size: 116.9 KiB, free: 4.4 GiB)
25/11/26 06:58:40 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:58:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.139.64.4:60904
25/11/26 06:58:40 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 2) in 367 ms on 10.139.64.4 (executor 0) (1/1)
25/11/26 06:58:40 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 2081346830872482017
25/11/26 06:58:40 INFO DAGScheduler: ResultStage 5 (first at Snapshot.scala:271) finished in 0.455 s
25/11/26 06:58:40 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
25/11/26 06:58:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
25/11/26 06:58:40 INFO DAGScheduler: Job 2 finished: first at Snapshot.scala:271, took 0.472300 s
25/11/26 06:58:41 INFO CodeGenerator: Code generated in 42.732527 ms
25/11/26 06:58:41 INFO QueryProfileListener: Query profile sent to logger, seq number: 1, app id: app-20251126065334-0000
25/11/26 06:58:41 INFO ParquetUtils: Using user defined output committer for Parquet: org.apache.spark.sql.parquet.DirectParquetOutputCommitter
25/11/26 06:58:41 INFO CodeGenerator: Code generated in 10.011054 ms
25/11/26 06:58:41 INFO SparkContext: Starting job: write at WriteIntoDeltaCommand.scala:92
25/11/26 06:58:41 INFO DAGScheduler: Got job 3 (write at WriteIntoDeltaCommand.scala:92) with 1 output partitions
25/11/26 06:58:41 INFO DAGScheduler: Final stage: ResultStage 6 (write at WriteIntoDeltaCommand.scala:92)
25/11/26 06:58:41 INFO DAGScheduler: Parents of final stage: List()
25/11/26 06:58:41 INFO DAGScheduler: Missing parents: List()
25/11/26 06:58:41 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[17] at write at WriteIntoDeltaCommand.scala:92), which has no missing parents
25/11/26 06:58:41 INFO DAGScheduler: Jars for session None: Map()
25/11/26 06:58:41 INFO DAGScheduler: Files for session None: Map()
25/11/26 06:58:41 INFO DAGScheduler: Archives for session None: Map()
25/11/26 06:58:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[17] at write at WriteIntoDeltaCommand.scala:92) (first 15 tasks are for partitions Vector(0))
25/11/26 06:58:41 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
25/11/26 06:58:41 INFO FairSchedulableBuilder: Added task set TaskSet_6.0 tasks to pool 2081346830872482017
25/11/26 06:58:41 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (10.139.64.4, executor 0, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
25/11/26 06:58:41 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 248.0 KiB, free 4.4 GiB)
25/11/26 06:58:41 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 94.6 KiB, free 4.4 GiB)
25/11/26 06:58:41 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.139.64.5:39807 (size: 94.6 KiB, free: 4.4 GiB)
25/11/26 06:58:41 INFO SparkContext: Created broadcast 8 from broadcast at TaskSetManager.scala:638
25/11/26 06:58:41 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.139.64.4:42657 (size: 94.6 KiB, free: 4.4 GiB)
25/11/26 06:58:43 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:58:44 INFO DriverCorral: Received SAFEr configs with version 1764127249701
25/11/26 06:58:44 INFO DriverCorral: DBFS health check ok
25/11/26 06:58:44 INFO SecuredHiveExternalCatalog: creating hiveClient from java.lang.Throwable
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:81)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:79)
	at org.apache.spark.sql.hive.HiveExternalCatalog.maybeSynchronized(HiveExternalCatalog.scala:115)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$1(HiveExternalCatalog.scala:155)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:397)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:154)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:326)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:309)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:299)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$new$5(DriverCorral.scala:485)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:100)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:105)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:104)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$new$4(DriverCorral.scala:485)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$new$4$adapted(DriverCorral.scala:484)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$new$3(DriverCorral.scala:484)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionContext(NamedTimer.scala:95)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionTags(NamedTimer.scala:95)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)
	at com.databricks.threading.NamedTimer$$anon$1.recordOperationWithResultTags(NamedTimer.scala:95)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)
	at com.databricks.threading.NamedTimer$$anon$1.recordOperation(NamedTimer.scala:95)
	at com.databricks.threading.NamedTimer$$anon$1.$anonfun$run$2(NamedTimer.scala:104)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionContext(NamedTimer.scala:95)
	at com.databricks.logging.UsageLogging.disableTracing(UsageLogging.scala:1380)
	at com.databricks.logging.UsageLogging.disableTracing$(UsageLogging.scala:1379)
	at com.databricks.threading.NamedTimer$$anon$1.disableTracing(NamedTimer.scala:95)
	at com.databricks.threading.NamedTimer$$anon$1.$anonfun$run$1(NamedTimer.scala:103)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.util.UntrustedUtils$.tryLog(UntrustedUtils.scala:109)
	at com.databricks.threading.NamedTimer$$anon$1.run(NamedTimer.scala:102)
	at java.util.TimerThread.mainLoop(Timer.java:555)
	at java.util.TimerThread.run(Timer.java:505)

25/11/26 06:58:44 ERROR CommandLineHelper$: Command [REDACTED] failed with exit code 1 out: err:
25/11/26 06:58:44 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
25/11/26 06:58:44 INFO HiveUtils: Initializing HiveMetastoreConnection version 0.13.0 using file:/databricks/databricks-hive/----ws_3_3--mvn--hadoop3--org.apache.logging.log4j--log4j-slf4j-impl--org.apache.logging.log4j__log4j-slf4j-impl__2.18.0.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.geronimo.specs--geronimo-jaspic_1.0_spec--org.apache.geronimo.specs__geronimo-jaspic_1.0_spec__1.0.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.codehaus.groovy--groovy-all--org.codehaus.groovy__groovy-all__2.1.6.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--jline--jline--jline__jline__0.9.94.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--com.esotericsoftware.minlog--minlog--com.esotericsoftware.minlog__minlog__1.2.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--commons-codec--commons-codec--commons-codec__commons-codec__1.8.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-common--org.apache.hive__hive-common__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--commons-io--commons-io--commons-io__commons-io__2.5.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--oro--oro--oro__oro__2.0.8.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--antlr--antlr--antlr__antlr__2.7.7.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.eclipse.jetty.aggregate--jetty-all--org.eclipse.jetty.aggregate__jetty-all__7.6.0.v20120127.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.derby--derby--org.apache.derby__derby__10.10.1.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.4.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-metastore--org.apache.hive__hive-metastore__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--stax--stax-api--stax__stax-api__1.0.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--com.zaxxer--HikariCP--com.zaxxer__HikariCP__2.5.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--com.esotericsoftware.reflectasm--reflectasm-shaded--com.esotericsoftware.reflectasm__reflectasm-shaded__1.07.jar:file:/databricks/databricks-hive/----ws_3_3--mvn--hadoop3--org.apache.logging.log4j--log4j-1.2-api--org.apache.logging.log4j__log4j-1.2-api__2.18.0.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-jdbc--org.apache.hive__hive-jdbc__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-exec--org.apache.hive__hive-exec__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--javax.transaction--jta--javax.transaction__jta__1.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.ow2.asm--asm--org.ow2.asm__asm__4.0.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.geronimo.specs--geronimo-annotation_1.0_spec--org.apache.geronimo.specs__geronimo-annotation_1.0_spec__1.1.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.objenesis--objenesis--org.objenesis__objenesis__1.2.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-beeline--org.apache.hive__hive-beeline__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-ant--org.apache.hive__hive-ant__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.0.5.jar:file:/databricks/databricks-hive/----ws_3_3--mvn--hadoop3--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.36.jar:file:/databricks/databricks-hive/----ws_3_3--mvn--hadoop3--org.apache.logging.log4j--log4j-api--org.apache.logging.log4j__log4j-api__2.18.0.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--javax.activation--activation--javax.activation__activation__1.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.geronimo.specs--geronimo-jta_1.1_spec--org.apache.geronimo.specs__geronimo-jta_1.1_spec__1.1.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-0.23--org.apache.hive.shims__hive-shims-0.23__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.2.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__4.1.19.jar:file:/databricks/databricks-hive/manifest.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--asm--asm--asm__asm__3.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.mortbay.jetty--jetty--org.mortbay.jetty__jetty__6.1.26.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-shims--org.apache.hive__hive-shims__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.avro--avro--org.apache.avro__avro__1.7.5.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.ant--ant--org.apache.ant__ant__1.9.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-0.20S--org.apache.hive.shims__hive-shims-0.20S__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__1.3.9.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--asm--asm-tree--asm__asm-tree__3.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.5.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.datanucleus--javax.jdo--org.datanucleus__javax.jdo__3.2.0-m3.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--commons-lang--commons-lang--commons-lang__commons-lang__2.4.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.0.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.mortbay.jetty--servlet-api--org.mortbay.jetty__servlet-api__2.5-20081211.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--asm--asm-commons--asm__asm-commons__3.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--javax.mail--mail--javax.mail__mail__1.4.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--junit--junit--junit__junit__3.8.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-0.20--org.apache.hive.shims__hive-shims-0.20__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.4.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-service--org.apache.hive__hive-service__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__4.1.17.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-serde--org.apache.hive__hive-serde__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--com.google.guava--guava--com.google.guava__guava__11.0.2.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:file:/databricks/databricks-hive/----ws_3_3--mvn--hadoop3--org.apache.logging.log4j--log4j-core--org.apache.logging.log4j__log4j-core__2.18.0.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.9.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--javax.servlet--servlet-api--javax.servlet__servlet-api__2.5.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-cli--org.apache.hive__hive-cli__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--com.esotericsoftware.kryo--kryo--com.esotericsoftware.kryo__kryo__2.21.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.0.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__4.2.4.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-common-secure--org.apache.hive.shims__hive-shims-common-secure__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.mortbay.jetty--jetty-util--org.mortbay.jetty__jetty-util__6.1.26.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.2.5.jar:file:/databricks/databricks-hive/----ws_3_3--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-common--org.apache.hive.shims__hive-shims-common__0.13.1-databricks-10.jar:file:/databricks/databricks-hive/bonecp-configs.jar
25/11/26 06:58:44 INFO PoolingHiveClient: Hive metastore connection pool implementation is HikariCP
25/11/26 06:58:44 INFO LocalHiveClientsPool: Create Hive Metastore client pool of size 20
25/11/26 06:58:44 INFO HiveClientImpl: Warehouse location for Hive client (version 0.13.1) is dbfs:/user/hive/warehouse
25/11/26 06:58:45 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
25/11/26 06:58:45 INFO ObjectStore: ObjectStore, initialize called
25/11/26 06:58:45 INFO Persistence: Property datanucleus.fixedDatastore unknown - will be ignored
25/11/26 06:58:45 INFO Persistence: Property datanucleus.connectionPool.idleTimeout unknown - will be ignored
25/11/26 06:58:45 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
25/11/26 06:58:45 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
25/11/26 06:58:45 INFO HikariDataSource: HikariPool-1 - Started.
25/11/26 06:58:45 INFO HikariDataSource: HikariPool-2 - Started.
25/11/26 06:58:46 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
25/11/26 06:58:46 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:58:47 INFO ObjectStore: Initialized ObjectStore
25/11/26 06:58:47 INFO HiveMetaStore: Added admin role in metastore
25/11/26 06:58:47 INFO HiveMetaStore: Added public role in metastore
25/11/26 06:58:47 INFO HiveMetaStore: No user is added in admin role, since config is empty
25/11/26 06:58:47 INFO HiveMetaStore: 0: get_database: default
25/11/26 06:58:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
25/11/26 06:58:47 INFO HiveMetaStore: 0: get_database: default
25/11/26 06:58:47 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
25/11/26 06:58:47 INFO DriverCorral: Metastore health check ok
25/11/26 06:58:49 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:58:52 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:58:55 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:58:58 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:59:01 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:59:04 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:59:07 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:59:10 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:59:13 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:59:16 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:59:19 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:59:22 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:59:25 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:59:28 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:59:31 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:59:34 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:59:36 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 54323 ms on 10.139.64.4 (executor 0) (1/1)
25/11/26 06:59:36 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 2081346830872482017
25/11/26 06:59:36 INFO DAGScheduler: ResultStage 6 (write at WriteIntoDeltaCommand.scala:92) finished in 54.354 s
25/11/26 06:59:36 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
25/11/26 06:59:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
25/11/26 06:59:36 INFO DAGScheduler: Job 3 finished: write at WriteIntoDeltaCommand.scala:92, took 54.358590 s
25/11/26 06:59:36 INFO FileFormatWriter: Start to commit write Job a9b4a119-d834-41b4-87e2-4ed71ddddd3c.
25/11/26 06:59:36 INFO FileSizeAutoTuner: File size tuning result: {"tuningType":"autoTuned","tunedConfs":{"spark.databricks.delta.optimize.minFileSize":"268435456","spark.databricks.delta.autoCompact.maxFileSize":"16777216","spark.databricks.delta.optimize.maxFileSize":"268435456","spark.databricks.delta.autoCompact.minFileSize":"8388608"}}
25/11/26 06:59:36 INFO FileFormatWriter: Write Job a9b4a119-d834-41b4-87e2-4ed71ddddd3c committed. Elapsed time: 160 ms.
25/11/26 06:59:36 INFO FileFormatWriter: Finished processing stats for write job a9b4a119-d834-41b4-87e2-4ed71ddddd3c.
25/11/26 06:59:36 INFO SparkContext: Starting job: collect at TransactionalWriteEdge.scala:587
25/11/26 06:59:36 INFO DAGScheduler: Got job 4 (collect at TransactionalWriteEdge.scala:587) with 1 output partitions
25/11/26 06:59:36 INFO DAGScheduler: Final stage: ResultStage 7 (collect at TransactionalWriteEdge.scala:587)
25/11/26 06:59:36 INFO DAGScheduler: Parents of final stage: List()
25/11/26 06:59:36 INFO DAGScheduler: Missing parents: List()
25/11/26 06:59:36 INFO DAGScheduler: Submitting ResultStage 7 (SQLExecutionRDD[19] at toRdd at TransactionalWriteEdge.scala:587), which has no missing parents
25/11/26 06:59:36 INFO DAGScheduler: Jars for session None: Map()
25/11/26 06:59:36 INFO DAGScheduler: Files for session None: Map()
25/11/26 06:59:36 INFO DAGScheduler: Archives for session None: Map()
25/11/26 06:59:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (SQLExecutionRDD[19] at toRdd at TransactionalWriteEdge.scala:587) (first 15 tasks are for partitions Vector(0))
25/11/26 06:59:36 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
25/11/26 06:59:36 INFO FairSchedulableBuilder: Added task set TaskSet_7.0 tasks to pool 2081346830872482017
25/11/26 06:59:36 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 4) (10.139.64.4, executor 0, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
25/11/26 06:59:36 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 112.4 KiB, free 4.4 GiB)
25/11/26 06:59:36 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 4.4 GiB)
25/11/26 06:59:36 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.139.64.5:39807 (size: 35.6 KiB, free: 4.4 GiB)
25/11/26 06:59:36 INFO SparkContext: Created broadcast 9 from broadcast at TaskSetManager.scala:638
25/11/26 06:59:36 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.139.64.4:42657 (size: 35.6 KiB, free: 4.4 GiB)
25/11/26 06:59:36 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 4) in 35 ms on 10.139.64.4 (executor 0) (1/1)
25/11/26 06:59:36 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 2081346830872482017
25/11/26 06:59:36 INFO DAGScheduler: ResultStage 7 (collect at TransactionalWriteEdge.scala:587) finished in 0.048 s
25/11/26 06:59:36 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
25/11/26 06:59:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
25/11/26 06:59:36 INFO DAGScheduler: Job 4 finished: collect at TransactionalWriteEdge.scala:587, took 0.062085 s
25/11/26 06:59:36 INFO QueryProfileListener: Query profile sent to logger, seq number: 2, app id: app-20251126065334-0000
25/11/26 06:59:36 INFO OptimisticTransaction: [tableId=62bed5a3,txnId=aaff2d93] Attempting to commit version 11 with 2 actions with WriteSerializable isolation level
25/11/26 06:59:36 INFO AzureBlobFileSystem:V3: FS_OP_CREATE FILE[abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/__tmp_path_dir/.00000000000000000011.json.75c54517-1c5c-4855-96bc-efffbb70346a.tmp] Creating output stream; permission: { masked: rw-r--r--, unmasked: rw-rw-rw- }, overwrite: false, bufferSize: 65536
25/11/26 06:59:36 INFO RetryTolerableRenameFSDataOutputStream: Writing atomically to abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/00000000000000000011.json using temp file abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/__tmp_path_dir/.00000000000000000011.json.75c54517-1c5c-4855-96bc-efffbb70346a.tmp
25/11/26 06:59:36 INFO AbfsOutputStream: FS_OP_CREATE FILE[abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/__tmp_path_dir/.00000000000000000011.json.75c54517-1c5c-4855-96bc-efffbb70346a.tmp] Closing stream; size: 0
25/11/26 06:59:36 INFO AbfsOutputStream: FS_OP_CREATE FILE[abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/__tmp_path_dir/.00000000000000000011.json.75c54517-1c5c-4855-96bc-efffbb70346a.tmp] Upload complete; size: 1518. Sent 3 requests: (1125-113639-ndjs76hq------:6f8e0d1f-6729-4eec-b401-ada0b131846e:97de1a12-b907-47e4-83d9-7075be68f6d0:::CR:0, d6686d33-101f-0023-53a2-5ed910000000), (1125-113639-ndjs76hq------:4b6c0fd6-6615-41c2-b059-34f96d1fc22d:97de1a12-b907-47e4-83d9-7075be68f6d0::5c50a60dc088:WR:0, d6686d34-101f-0023-54a2-5ed910000000), (1125-113639-ndjs76hq------:ab97cc46-af42-4012-b740-26405daf95ac:97de1a12-b907-47e4-83d9-7075be68f6d0::5c50a60dc088:WR:0, d6686d35-101f-0023-55a2-5ed910000000)
25/11/26 06:59:36 INFO AzureBlobFileSystem:V3: FS_OP_RENAME SRC[abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/__tmp_path_dir/.00000000000000000011.json.75c54517-1c5c-4855-96bc-efffbb70346a.tmp] DST[abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/00000000000000000011.json] Starting rename. Issuing rename operation.
25/11/26 06:59:36 INFO AzureBlobFileSystem:V3: FS_OP_RENAME SRC[abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/__tmp_path_dir/.00000000000000000011.json.75c54517-1c5c-4855-96bc-efffbb70346a.tmp] DST[abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/00000000000000000011.json] Rename successful. Sent 1 requests: (1125-113639-ndjs76hq------:48b775d1-aa1b-4fdd-835a-d80d1f070bde:97de1a12-b907-47e4-83d9-7075be68f6d0:68d79686-60fd-4ade-9283-4841b9a32ba1::RN:0, d6686d36-101f-0023-56a2-5ed910000000)
25/11/26 06:59:36 INFO RetryTolerableRenameFSDataOutputStream: Renamed temp file abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/__tmp_path_dir/.00000000000000000011.json.75c54517-1c5c-4855-96bc-efffbb70346a.tmp to abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/00000000000000000011.json
25/11/26 06:59:36 INFO OptimisticTransaction: [tableId=62bed5a3,txnId=aaff2d93] Incremental commit: starting with snapshot version 10
25/11/26 06:59:36 INFO DeltaLog: Loading version 11 starting from checkpoint version 10.
25/11/26 06:59:36 INFO SnapshotEdge: [tableId=62bed5a3-0999-4586-90cd-7b6af6e36be7] Created snapshot SnapshotEdge(path=abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log, version=11, metadata=Metadata(62bed5a3-0999-4586-90cd-7b6af6e36be7,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{"scale":0}},{"name":"name","type":"string","nullable":true,"metadata":{"scale":0}},{"name":"age","type":"integer","nullable":true,"metadata":{"scale":0}},{"name":"grade","type":"decimal(4,2)","nullable":true,"metadata":{"scale":2}},{"name":"birthday","type":"date","nullable":true,"metadata":{"scale":0}},{"name":"logintime","type":"timestamp","nullable":true,"metadata":{"scale":6}}]},List(),Map(),Some(1764131452056)), logSegment=LogSegment(abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log,11,ArrayBuffer(VersionedFileStatus{VersionedFileStatus{path=abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/00000000000000000011.json; isDirectory=false; length=1518; replication=1; blocksize=268435456; modification_time=1764140376000; access_time=0; owner=b6eba1ca-db96-49f9-b19f-f309bce4121d; group=root; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}; version='"0x8DE2CB95C7EAE99"'}),WrappedArray(VersionedFileStatus{VersionedFileStatus{path=abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/00000000000000000010.checkpoint.parquet; isDirectory=false; length=25741; replication=1; blocksize=268435456; modification_time=1764139674000; access_time=0; owner=b6eba1ca-db96-49f9-b19f-f309bce4121d; group=$superuser; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}; version='0x8DE2CB7B9F4F209'}),Some(10),1764140376000), checksumOpt=Some(VersionChecksum(Some(aaff2d93-039a-4b90-a437-7eed51e8281c),3646547568,9,None,None,1,1,Some(Stream()),None,Metadata(62bed5a3-0999-4586-90cd-7b6af6e36be7,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{"scale":0}},{"name":"name","type":"string","nullable":true,"metadata":{"scale":0}},{"name":"age","type":"integer","nullable":true,"metadata":{"scale":0}},{"name":"grade","type":"decimal(4,2)","nullable":true,"metadata":{"scale":2}},{"name":"birthday","type":"date","nullable":true,"metadata":{"scale":0}},{"name":"logintime","type":"timestamp","nullable":true,"metadata":{"scale":6}}]},List(),Map(),Some(1764131452056)),Protocol(1,2),Some(FileSizeHistogram(Vector(0, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576, 2097152, 4194304, 8388608, 12582912, 16777216, 20971520, 25165824, 29360128, 33554432, 37748736, 41943040, 50331648, 58720256, 67108864, 75497472, 83886080, 92274688, 100663296, 109051904, 117440512, 125829120, 130023424, 134217728, 138412032, 142606336, 146800640, 150994944, 167772160, 184549376, 201326592, 218103808, 234881024, 251658240, 268435456, 285212672, 301989888, 318767104, 335544320, 352321536, 369098752, 385875968, 402653184, 419430400, 436207616, 452984832, 469762048, 486539264, 503316480, 520093696, 536870912, 553648128, 570425344, 587202560, 603979776, 671088640, 738197504, 805306368, 872415232, 939524096, 1006632960, 1073741824, 1140850688, 1207959552, 1275068416, 1342177280, 1409286144, 1476395008, 1610612736, 1744830464, 1879048192, 2013265920, 2147483648, 2415919104, 2684354560, 2952790016, 3221225472, 3489660928, 3758096384, 4026531840, 4294967296, 8589934592, 17179869184, 34359738368, 68719476736, 137438953472, 274877906944),[J@4f19e006,[J@38982bd8)),None,Some(Stream(AddFile(part-00000-884d449d-510d-499b-86b0-788d47e58357-c000.snappy.parquet,Map(),405171952,1764136774000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764136774000000, MIN_INSERTION_TIME -> 1764136774000000, MAX_INSERTION_TIME -> 1764136774000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-5e1c2199-b033-4e49-8d2e-d75a95e4871f-c000.snappy.parquet,Map(),405171952,1764135958000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764135958000000, MIN_INSERTION_TIME -> 1764135958000000, MAX_INSERTION_TIME -> 1764135958000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-c07e2cc6-f2ac-44bd-a972-b2db6f803a6e-c000.snappy.parquet,Map(),405171952,1764135057000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764135057000000, MIN_INSERTION_TIME -> 1764135057000000, MAX_INSERTION_TIME -> 1764135057000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-a650ea24-1236-4c21-a4f9-d04923945b4c-c000.snappy.parquet,Map(),405171952,1764140375000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764140375000000, MIN_INSERTION_TIME -> 1764140375000000, MAX_INSERTION_TIME -> 1764140375000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-e5f2304d-bad7-491d-ad4c-0e85916c37a6-c000.snappy.parquet,Map(),405171952,1764137105000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764137105000000, MIN_INSERTION_TIME -> 1764137105000000, MAX_INSERTION_TIME -> 1764137105000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-9bcc8d32-27ed-44cf-b137-a1cee0b2926e-c000.snappy.parquet,Map(),405171952,1764137553000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764137553000000, MIN_INSERTION_TIME -> 1764137553000000, MAX_INSERTION_TIME -> 1764137553000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-4a2e5635-bd9b-4892-9e39-c53414c85b07-c000.snappy.parquet,Map(),405171952,1764135429000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764135429000000, MIN_INSERTION_TIME -> 1764135429000000, MAX_INSERTION_TIME -> 1764135429000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-c15fd30b-8147-4e96-a419-0a7fe6f8953a-c000.snappy.parquet,Map(),405171952,1764136406000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764136406000000, MIN_INSERTION_TIME -> 1764136406000000, MAX_INSERTION_TIME -> 1764136406000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-93ed7c89-5969-4fcc-bc14-6c931e8bceaa-c000.snappy.parquet,Map(),405171952,1764139669000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764139669000000, MIN_INSERTION_TIME -> 1764139669000000, MAX_INSERTION_TIME -> 1764139669000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None))))))
25/11/26 06:59:36 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 436.7 KiB, free 4.4 GiB)
25/11/26 06:59:36 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 15.8 KiB, free 4.4 GiB)
25/11/26 06:59:36 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.139.64.5:39807 (size: 15.8 KiB, free: 4.4 GiB)
25/11/26 06:59:36 INFO SparkContext: Created broadcast 10 from broadcast at DeltaLog.scala:718
25/11/26 06:59:36 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(Parquet, numFilesInSegment: 1, totalFileSize: 25741)
25/11/26 06:59:36 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 1518)
25/11/26 06:59:37 INFO CodeGenerator: Code generated in 22.532192 ms
25/11/26 06:59:37 INFO CodeGenerator: Code generated in 35.069332 ms
25/11/26 06:59:37 INFO FileSourceStrategy: Pushed Filters: 
25/11/26 06:59:37 INFO FileSourceStrategy: Post-Scan Filters: 
25/11/26 06:59:37 INFO CodeGenerator: Code generated in 94.981912 ms
25/11/26 06:59:37 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 539.6 KiB, free 4.4 GiB)
25/11/26 06:59:37 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 4.4 GiB)
25/11/26 06:59:37 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.139.64.5:39807 (size: 19.5 KiB, free: 4.4 GiB)
25/11/26 06:59:37 INFO SparkContext: Created broadcast 11 from $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63
25/11/26 06:59:37 INFO FileSourceScanExec: Planning scan with bin packing, max split size: 134217728 bytes, max partition size: 4194304, open cost is considered as scanning 4194304 bytes.
25/11/26 06:59:37 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
25/11/26 06:59:37 INFO CodeGenerator: Code generated in 39.992762 ms
25/11/26 06:59:37 INFO DAGScheduler: Registering RDD 26 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) as input to shuffle 2
25/11/26 06:59:37 INFO DAGScheduler: Got map stage job 5 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) with 3 output partitions
25/11/26 06:59:37 INFO DAGScheduler: Final stage: ShuffleMapStage 8 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63)
25/11/26 06:59:37 INFO DAGScheduler: Parents of final stage: List()
25/11/26 06:59:37 INFO DAGScheduler: Missing parents: List()
25/11/26 06:59:37 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63), which has no missing parents
25/11/26 06:59:37 INFO DAGScheduler: Jars for session None: Map()
25/11/26 06:59:37 INFO DAGScheduler: Files for session None: Map()
25/11/26 06:59:37 INFO DAGScheduler: Archives for session None: Map()
25/11/26 06:59:37 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) (first 15 tasks are for partitions Vector(0, 1, 2))
25/11/26 06:59:37 INFO TaskSchedulerImpl: Adding task set 8.0 with 3 tasks resource profile 0
25/11/26 06:59:37 INFO FairSchedulableBuilder: Added task set TaskSet_8.0 tasks to pool 2081346830872482017
25/11/26 06:59:37 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (10.139.64.4, executor 0, partition 0, NODE_LOCAL, taskResourceAssignments Map())
25/11/26 06:59:37 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 6) (10.139.64.4, executor 0, partition 1, PROCESS_LOCAL, taskResourceAssignments Map())
25/11/26 06:59:37 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 7) (10.139.64.4, executor 0, partition 2, PROCESS_LOCAL, taskResourceAssignments Map())
25/11/26 06:59:37 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 294.2 KiB, free 4.4 GiB)
25/11/26 06:59:37 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 86.6 KiB, free 4.4 GiB)
25/11/26 06:59:37 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.139.64.5:39807 (size: 86.6 KiB, free: 4.4 GiB)
25/11/26 06:59:37 INFO SparkContext: Created broadcast 12 from broadcast at TaskSetManager.scala:638
25/11/26 06:59:37 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.139.64.4:42657 (size: 86.6 KiB, free: 4.4 GiB)
25/11/26 06:59:37 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 7) in 62 ms on 10.139.64.4 (executor 0) (1/3)
25/11/26 06:59:37 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 6) in 63 ms on 10.139.64.4 (executor 0) (2/3)
25/11/26 06:59:37 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.139.64.4:42657 (size: 19.5 KiB, free: 4.4 GiB)
25/11/26 06:59:37 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 226 ms on 10.139.64.4 (executor 0) (3/3)
25/11/26 06:59:37 INFO DAGScheduler: ShuffleMapStage 8 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) finished in 0.263 s
25/11/26 06:59:37 INFO DAGScheduler: looking for newly runnable stages
25/11/26 06:59:37 INFO DAGScheduler: running: Set()
25/11/26 06:59:37 INFO DAGScheduler: waiting: Set()
25/11/26 06:59:37 INFO DAGScheduler: failed: Set()
25/11/26 06:59:37 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 2081346830872482017
25/11/26 06:59:38 INFO CodeGenerator: Code generated in 98.175805 ms
25/11/26 06:59:38 INFO SQLAppStatusListener: Recording cache-related metrics in usage logs:
ioCacheWorkersDiskUsage={"10.139.64.4":{"diskUsage":4007,"lifetime":352458}}, ioCacheNumScanTasks={"numLocalScanTasks":2,"numNonLocalScanTasks":0}, ioCacheDiskUsageLimit=78696390656
25/11/26 06:59:38 INFO QueryProfileListener: Query profile sent to logger, seq number: 3, app id: app-20251126065334-0000
25/11/26 06:59:38 INFO CodeGenerator: Code generated in 49.202455 ms
25/11/26 06:59:38 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 8.8 KiB, free 4.4 GiB)
25/11/26 06:59:38 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 4.4 GiB)
25/11/26 06:59:38 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.139.64.5:39807 (size: 4.6 KiB, free: 4.4 GiB)
25/11/26 06:59:38 INFO SparkContext: Created broadcast 13 from $anonfun$executePhase$2 at LexicalThreadLocal.scala:63
25/11/26 06:59:38 INFO CodeGenerator: Code generated in 81.237844 ms
25/11/26 06:59:38 INFO SparkContext: Starting job: collect at Checksum.scala:578
25/11/26 06:59:38 INFO DAGScheduler: Got job 6 (collect at Checksum.scala:578) with 1 output partitions
25/11/26 06:59:38 INFO DAGScheduler: Final stage: ResultStage 10 (collect at Checksum.scala:578)
25/11/26 06:59:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
25/11/26 06:59:38 INFO DAGScheduler: Missing parents: List()
25/11/26 06:59:38 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[37] at collect at Checksum.scala:578), which has no missing parents
25/11/26 06:59:38 INFO DAGScheduler: Jars for session None: Map()
25/11/26 06:59:38 INFO DAGScheduler: Files for session None: Map()
25/11/26 06:59:38 INFO DAGScheduler: Archives for session None: Map()
25/11/26 06:59:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[37] at collect at Checksum.scala:578) (first 15 tasks are for partitions Vector(0))
25/11/26 06:59:38 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
25/11/26 06:59:38 INFO FairSchedulableBuilder: Added task set TaskSet_10.0 tasks to pool 2081346830872482017
25/11/26 06:59:38 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (10.139.64.4, executor 0, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
25/11/26 06:59:38 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 477.4 KiB, free 4.4 GiB)
25/11/26 06:59:38 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 134.9 KiB, free 4.4 GiB)
25/11/26 06:59:38 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.139.64.5:39807 (size: 134.9 KiB, free: 4.4 GiB)
25/11/26 06:59:38 INFO SparkContext: Created broadcast 14 from broadcast at TaskSetManager.scala:638
25/11/26 06:59:38 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.139.64.4:42657 (size: 134.9 KiB, free: 4.4 GiB)
25/11/26 06:59:39 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.139.64.4:60904
25/11/26 06:59:39 INFO BlockManagerInfo: Added rdd_33_0 in memory on 10.139.64.4:42657 (size: 1870.0 B, free: 4.4 GiB)
25/11/26 06:59:39 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.139.64.4:42657 (size: 4.6 KiB, free: 4.4 GiB)
25/11/26 06:59:39 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 565 ms on 10.139.64.4 (executor 0) (1/1)
25/11/26 06:59:39 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 2081346830872482017
25/11/26 06:59:39 INFO DAGScheduler: ResultStage 10 (collect at Checksum.scala:578) finished in 0.782 s
25/11/26 06:59:39 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
25/11/26 06:59:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
25/11/26 06:59:39 INFO DAGScheduler: Job 6 finished: collect at Checksum.scala:578, took 0.798234 s
25/11/26 06:59:39 INFO CodeGenerator: Code generated in 32.658886 ms
25/11/26 06:59:39 INFO QueryProfileListener: Query profile sent to logger, seq number: 4, app id: app-20251126065334-0000
25/11/26 06:59:39 INFO DAGScheduler: Registering RDD 40 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) as input to shuffle 3
25/11/26 06:59:39 INFO DAGScheduler: Got map stage job 7 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) with 1 output partitions
25/11/26 06:59:39 INFO DAGScheduler: Final stage: ShuffleMapStage 12 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63)
25/11/26 06:59:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
25/11/26 06:59:39 INFO DAGScheduler: Missing parents: List()
25/11/26 06:59:39 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[40] at $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63), which has no missing parents
25/11/26 06:59:39 INFO DAGScheduler: Jars for session None: Map()
25/11/26 06:59:39 INFO DAGScheduler: Files for session None: Map()
25/11/26 06:59:39 INFO DAGScheduler: Archives for session None: Map()
25/11/26 06:59:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[40] at $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) (first 15 tasks are for partitions Vector(0))
25/11/26 06:59:39 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
25/11/26 06:59:39 INFO FairSchedulableBuilder: Added task set TaskSet_12.0 tasks to pool 2081346830872482017
25/11/26 06:59:39 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (10.139.64.4, executor 0, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
25/11/26 06:59:39 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 448.4 KiB, free 4.4 GiB)
25/11/26 06:59:39 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 129.2 KiB, free 4.4 GiB)
25/11/26 06:59:39 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.139.64.5:39807 (size: 129.2 KiB, free: 4.4 GiB)
25/11/26 06:59:39 INFO SparkContext: Created broadcast 15 from broadcast at TaskSetManager.scala:638
25/11/26 06:59:39 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.139.64.4:42657 (size: 129.2 KiB, free: 4.4 GiB)
25/11/26 06:59:39 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 76 ms on 10.139.64.4 (executor 0) (1/1)
25/11/26 06:59:39 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 2081346830872482017
25/11/26 06:59:39 INFO DAGScheduler: ShuffleMapStage 12 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) finished in 0.141 s
25/11/26 06:59:39 INFO DAGScheduler: looking for newly runnable stages
25/11/26 06:59:39 INFO DAGScheduler: running: Set()
25/11/26 06:59:39 INFO DAGScheduler: waiting: Set()
25/11/26 06:59:39 INFO DAGScheduler: failed: Set()
25/11/26 06:59:39 INFO SparkContext: Starting job: first at Snapshot.scala:271
25/11/26 06:59:39 INFO DAGScheduler: Got job 8 (first at Snapshot.scala:271) with 1 output partitions
25/11/26 06:59:39 INFO DAGScheduler: Final stage: ResultStage 15 (first at Snapshot.scala:271)
25/11/26 06:59:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
25/11/26 06:59:39 INFO DAGScheduler: Missing parents: List()
25/11/26 06:59:39 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[42] at first at Snapshot.scala:271), which has no missing parents
25/11/26 06:59:39 INFO DAGScheduler: Jars for session None: Map()
25/11/26 06:59:39 INFO DAGScheduler: Files for session None: Map()
25/11/26 06:59:39 INFO DAGScheduler: Archives for session None: Map()
25/11/26 06:59:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[42] at first at Snapshot.scala:271) (first 15 tasks are for partitions Vector(0))
25/11/26 06:59:39 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
25/11/26 06:59:39 INFO FairSchedulableBuilder: Added task set TaskSet_15.0 tasks to pool 2081346830872482017
25/11/26 06:59:39 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (10.139.64.4, executor 0, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
25/11/26 06:59:39 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 393.3 KiB, free 4.4 GiB)
25/11/26 06:59:39 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 120.3 KiB, free 4.4 GiB)
25/11/26 06:59:39 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.139.64.5:39807 (size: 120.3 KiB, free: 4.4 GiB)
25/11/26 06:59:39 INFO SparkContext: Created broadcast 16 from broadcast at TaskSetManager.scala:638
25/11/26 06:59:39 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.139.64.4:42657 (size: 120.3 KiB, free: 4.4 GiB)
25/11/26 06:59:39 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.139.64.4:60904
25/11/26 06:59:39 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 74 ms on 10.139.64.4 (executor 0) (1/1)
25/11/26 06:59:39 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 2081346830872482017
25/11/26 06:59:39 INFO DAGScheduler: ResultStage 15 (first at Snapshot.scala:271) finished in 0.129 s
25/11/26 06:59:39 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
25/11/26 06:59:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
25/11/26 06:59:39 INFO DAGScheduler: Job 8 finished: first at Snapshot.scala:271, took 0.136750 s
25/11/26 06:59:39 INFO DeltaLog: Updated snapshot to SnapshotEdge(path=abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log, version=11, metadata=Metadata(62bed5a3-0999-4586-90cd-7b6af6e36be7,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{"scale":0}},{"name":"name","type":"string","nullable":true,"metadata":{"scale":0}},{"name":"age","type":"integer","nullable":true,"metadata":{"scale":0}},{"name":"grade","type":"decimal(4,2)","nullable":true,"metadata":{"scale":2}},{"name":"birthday","type":"date","nullable":true,"metadata":{"scale":0}},{"name":"logintime","type":"timestamp","nullable":true,"metadata":{"scale":6}}]},List(),Map(),Some(1764131452056)), logSegment=LogSegment(abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log,11,ArrayBuffer(VersionedFileStatus{VersionedFileStatus{path=abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/00000000000000000011.json; isDirectory=false; length=1518; replication=1; blocksize=268435456; modification_time=1764140376000; access_time=0; owner=b6eba1ca-db96-49f9-b19f-f309bce4121d; group=root; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}; version='"0x8DE2CB95C7EAE99"'}),WrappedArray(VersionedFileStatus{VersionedFileStatus{path=abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/00000000000000000010.checkpoint.parquet; isDirectory=false; length=25741; replication=1; blocksize=268435456; modification_time=1764139674000; access_time=0; owner=b6eba1ca-db96-49f9-b19f-f309bce4121d; group=$superuser; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}; version='0x8DE2CB7B9F4F209'}),Some(10),1764140376000), checksumOpt=Some(VersionChecksum(Some(aaff2d93-039a-4b90-a437-7eed51e8281c),3646547568,9,None,None,1,1,Some(Stream()),None,Metadata(62bed5a3-0999-4586-90cd-7b6af6e36be7,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{"scale":0}},{"name":"name","type":"string","nullable":true,"metadata":{"scale":0}},{"name":"age","type":"integer","nullable":true,"metadata":{"scale":0}},{"name":"grade","type":"decimal(4,2)","nullable":true,"metadata":{"scale":2}},{"name":"birthday","type":"date","nullable":true,"metadata":{"scale":0}},{"name":"logintime","type":"timestamp","nullable":true,"metadata":{"scale":6}}]},List(),Map(),Some(1764131452056)),Protocol(1,2),Some(FileSizeHistogram(Vector(0, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576, 2097152, 4194304, 8388608, 12582912, 16777216, 20971520, 25165824, 29360128, 33554432, 37748736, 41943040, 50331648, 58720256, 67108864, 75497472, 83886080, 92274688, 100663296, 109051904, 117440512, 125829120, 130023424, 134217728, 138412032, 142606336, 146800640, 150994944, 167772160, 184549376, 201326592, 218103808, 234881024, 251658240, 268435456, 285212672, 301989888, 318767104, 335544320, 352321536, 369098752, 385875968, 402653184, 419430400, 436207616, 452984832, 469762048, 486539264, 503316480, 520093696, 536870912, 553648128, 570425344, 587202560, 603979776, 671088640, 738197504, 805306368, 872415232, 939524096, 1006632960, 1073741824, 1140850688, 1207959552, 1275068416, 1342177280, 1409286144, 1476395008, 1610612736, 1744830464, 1879048192, 2013265920, 2147483648, 2415919104, 2684354560, 2952790016, 3221225472, 3489660928, 3758096384, 4026531840, 4294967296, 8589934592, 17179869184, 34359738368, 68719476736, 137438953472, 274877906944),[J@4f19e006,[J@38982bd8)),None,Some(Stream(AddFile(part-00000-884d449d-510d-499b-86b0-788d47e58357-c000.snappy.parquet,Map(),405171952,1764136774000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764136774000000, MIN_INSERTION_TIME -> 1764136774000000, MAX_INSERTION_TIME -> 1764136774000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-5e1c2199-b033-4e49-8d2e-d75a95e4871f-c000.snappy.parquet,Map(),405171952,1764135958000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764135958000000, MIN_INSERTION_TIME -> 1764135958000000, MAX_INSERTION_TIME -> 1764135958000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-c07e2cc6-f2ac-44bd-a972-b2db6f803a6e-c000.snappy.parquet,Map(),405171952,1764135057000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764135057000000, MIN_INSERTION_TIME -> 1764135057000000, MAX_INSERTION_TIME -> 1764135057000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-a650ea24-1236-4c21-a4f9-d04923945b4c-c000.snappy.parquet,Map(),405171952,1764140375000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764140375000000, MIN_INSERTION_TIME -> 1764140375000000, MAX_INSERTION_TIME -> 1764140375000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-e5f2304d-bad7-491d-ad4c-0e85916c37a6-c000.snappy.parquet,Map(),405171952,1764137105000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764137105000000, MIN_INSERTION_TIME -> 1764137105000000, MAX_INSERTION_TIME -> 1764137105000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-9bcc8d32-27ed-44cf-b137-a1cee0b2926e-c000.snappy.parquet,Map(),405171952,1764137553000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764137553000000, MIN_INSERTION_TIME -> 1764137553000000, MAX_INSERTION_TIME -> 1764137553000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-4a2e5635-bd9b-4892-9e39-c53414c85b07-c000.snappy.parquet,Map(),405171952,1764135429000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764135429000000, MIN_INSERTION_TIME -> 1764135429000000, MAX_INSERTION_TIME -> 1764135429000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-c15fd30b-8147-4e96-a419-0a7fe6f8953a-c000.snappy.parquet,Map(),405171952,1764136406000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764136406000000, MIN_INSERTION_TIME -> 1764136406000000, MAX_INSERTION_TIME -> 1764136406000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None), AddFile(part-00000-93ed7c89-5969-4fcc-bc14-6c931e8bceaa-c000.snappy.parquet,Map(),405171952,1764139669000,false,{"numRecords":10000473,"minValues":{"id":1,"name":"0000012a96324ac00214c0d142e80c73","age":0,"grade":0.00,"birthday":"2019-02-11","logintime":"2021-11-07T12:22:09.626Z"},"maxValues":{"id":10000370,"name":"ffffff6bb2702654c32939c89b135071","age":100,"grade":99.00,"birthday":"2022-01-18","logintime":"2022-01-20T16:07:50.220Z"},"nullCount":{"id":0,"name":0,"age":0,"grade":0,"birthday":0,"logintime":0}},Map(INSERTION_TIME -> 1764139669000000, MIN_INSERTION_TIME -> 1764139669000000, MAX_INSERTION_TIME -> 1764139669000000, OPTIMIZE_TARGET_SIZE -> 268435456),null,None))))))
25/11/26 06:59:39 INFO MapPartitionsRDD: Removing RDD 10 from persistence list
25/11/26 06:59:39 INFO QueryProfileListener: Query profile sent to logger, seq number: 5, app id: app-20251126065334-0000
25/11/26 06:59:39 INFO BlockManager: Removing RDD 10
25/11/26 06:59:39 INFO FileSizeAutoTuner: File size tuning result: {"tuningType":"autoTuned","tunedConfs":{"spark.databricks.delta.optimize.minFileSize":"268435456","spark.databricks.delta.autoCompact.maxFileSize":"16777216","spark.databricks.delta.optimize.maxFileSize":"268435456","spark.databricks.delta.autoCompact.minFileSize":"8388608"}}
25/11/26 06:59:40 INFO OptimisticTransaction: [tableId=62bed5a3,txnId=aaff2d93] Committed delta #11 to abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log
25/11/26 06:59:40 INFO ChecksumHook: Writing checksum file for table path abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log version 11
25/11/26 06:59:40 INFO AzureBlobFileSystem:V3: FS_OP_CREATE FILE[abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/__tmp_path_dir/.00000000000000000011.crc.62a40df4-e145-479d-9a3c-7d7ed4fbeaf8.tmp] Creating output stream; permission: { masked: rw-r--r--, unmasked: rw-rw-rw- }, overwrite: false, bufferSize: 65536
25/11/26 06:59:40 INFO RetryTolerableRenameFSDataOutputStream: Writing atomically to abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/00000000000000000011.crc using temp file abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/__tmp_path_dir/.00000000000000000011.crc.62a40df4-e145-479d-9a3c-7d7ed4fbeaf8.tmp
25/11/26 06:59:40 INFO AbfsOutputStream: FS_OP_CREATE FILE[abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/__tmp_path_dir/.00000000000000000011.crc.62a40df4-e145-479d-9a3c-7d7ed4fbeaf8.tmp] Closing stream; size: 0
25/11/26 06:59:40 INFO AbfsOutputStream: FS_OP_CREATE FILE[abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/__tmp_path_dir/.00000000000000000011.crc.62a40df4-e145-479d-9a3c-7d7ed4fbeaf8.tmp] Upload complete; size: 9530. Sent 3 requests: (1125-113639-ndjs76hq------:dc24477d-be24-49b2-a42e-30c0af2343dd:97de1a12-b907-47e4-83d9-7075be68f6d0:::CR:0, d6686d55-101f-0023-6da2-5ed910000000), (1125-113639-ndjs76hq------:7820a105-dace-4cfb-9996-7ede81e35576:97de1a12-b907-47e4-83d9-7075be68f6d0::2297cc26f2b4:WR:0, d6686d56-101f-0023-6ea2-5ed910000000), (1125-113639-ndjs76hq------:b570a869-7f6b-4e3d-a689-2db13cf54274:97de1a12-b907-47e4-83d9-7075be68f6d0::2297cc26f2b4:WR:0, d6686d57-101f-0023-6fa2-5ed910000000)
25/11/26 06:59:40 INFO AzureBlobFileSystem:V3: FS_OP_RENAME SRC[abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/__tmp_path_dir/.00000000000000000011.crc.62a40df4-e145-479d-9a3c-7d7ed4fbeaf8.tmp] DST[abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/00000000000000000011.crc] Starting rename. Issuing rename operation.
25/11/26 06:59:40 INFO AzureBlobFileSystem:V3: FS_OP_RENAME SRC[abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/__tmp_path_dir/.00000000000000000011.crc.62a40df4-e145-479d-9a3c-7d7ed4fbeaf8.tmp] DST[abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/00000000000000000011.crc] Rename successful. Sent 1 requests: (1125-113639-ndjs76hq------:92311d80-2e14-4a81-af85-44b3f909126b:97de1a12-b907-47e4-83d9-7075be68f6d0:3a31aa61-4729-4d00-bd93-4845c4466d2a::RN:0, d6686d58-101f-0023-70a2-5ed910000000)
25/11/26 06:59:40 INFO RetryTolerableRenameFSDataOutputStream: Renamed temp file abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/__tmp_path_dir/.00000000000000000011.crc.62a40df4-e145-479d-9a3c-7d7ed4fbeaf8.tmp to abfss://uctarhone@tarhonemetastore.dfs.core.chinacloudapi.cn/tarhoneroot1/bronze/test/vwtable1/_delta_log/00000000000000000011.crc
25/11/26 06:59:40 INFO ClusterLoadMonitor: Removed query with execution ID:0. Current active queries:0
25/11/26 06:59:40 INFO QueryProfileListener: Query profile sent to logger, seq number: 6, app id: app-20251126065334-0000
25/11/26 06:59:40 INFO SparkContext: SparkContext is stopping from stop at NativeMethodAccessorImpl.java:0.
25/11/26 06:59:40 INFO HiveServer2: Shutting down HiveServer2
25/11/26 06:59:40 INFO ThriftCLIService: Caught InterruptedException. Shutting down thrift server.
25/11/26 06:59:40 INFO AbstractConnector: Stopped ServerConnector@3bf1aba7{SSL, (ssl, http/1.1)}{0.0.0.0:10000}
25/11/26 06:59:40 INFO session: node0 Stopped scavenging
25/11/26 06:59:40 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@5022aaa3{/,null,STOPPED}
25/11/26 06:59:40 INFO AbstractConnector: Stopped Spark@641bd93e{HTTP/1.1, (http/1.1)}{10.139.64.5:40001}
25/11/26 06:59:40 INFO SparkUI: Stopped Spark web UI at http://10.139.64.5:40001
25/11/26 06:59:40 INFO ThriftCLIService: Thrift HTTP server has been stopped
25/11/26 06:59:40 INFO AbstractService: Service:ThriftHttpCLIService is stopped.
25/11/26 06:59:40 INFO AbstractService: Service:OperationManager is stopped.
25/11/26 06:59:40 INFO AbstractService: Service:SessionManager is stopped.
25/11/26 06:59:40 INFO AbstractService: Service:CLIService is stopped.
25/11/26 06:59:40 INFO AbstractService: Service:HiveServer2 is stopped.
25/11/26 06:59:40 INFO StandaloneSchedulerBackend: Shutting down all executors
25/11/26 06:59:40 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
25/11/26 06:59:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/11/26 06:59:40 INFO MemoryStore: MemoryStore cleared
25/11/26 06:59:40 INFO BlockManager: BlockManager stopped
25/11/26 06:59:40 INFO BlockManagerMaster: BlockManagerMaster stopped
25/11/26 06:59:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/11/26 06:59:40 INFO SparkContext: Successfully stopped SparkContext
25/11/26 06:59:40 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 1.0, New Ema: 0.85 
25/11/26 06:59:43 WARN DriverDaemon: Unexpected exception: java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.spark.sql.internal.SharedState.getSchedulerStats(SharedState.scala:416)
	at org.apache.spark.sql.SQLContext.getSchedulerStats(SQLContext.scala:768)
	at com.databricks.backend.daemon.driver.DriverCorral$.getAutoscalingInfo(DriverCorral.scala:1712)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRPCRequest(DriverCorral.scala:1079)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:1128)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:1126)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:119)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:116)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:24)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:24)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)
	at com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:24)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:24)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:115)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:958)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:958)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:874)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2(JettyServer.scala:503)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2$adapted(JettyServer.scala:478)
	at com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:387)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)
	at com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)
	at com.databricks.logging.activity.ActivityContextFactory$.withAttributionTags(ActivityContextFactory.scala:53)
	at com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:381)
	at com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:164)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:478)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:375)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:523)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
	at com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:80)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.lang.Thread.run(Thread.java:750)
25/11/26 06:59:43 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 0.85, New Ema: 0.0 
25/11/26 06:59:44 INFO DriverCorral: Received SAFEr configs with version 1764127249701
25/11/26 06:59:44 ERROR CommandLineHelper$: Command [REDACTED] failed with exit code 1 out: err:
25/11/26 06:59:48 WARN DriverDaemon: Unexpected exception: java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.spark.sql.internal.SharedState.getSchedulerStats(SharedState.scala:416)
	at org.apache.spark.sql.SQLContext.getSchedulerStats(SQLContext.scala:768)
	at com.databricks.backend.daemon.driver.DriverCorral$.getAutoscalingInfo(DriverCorral.scala:1712)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRPCRequest(DriverCorral.scala:1079)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:1128)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:1126)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:119)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:116)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:24)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:24)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)
	at com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:24)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:24)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:115)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:958)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:958)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:874)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2(JettyServer.scala:503)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2$adapted(JettyServer.scala:478)
	at com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:387)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)
	at com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)
	at com.databricks.logging.activity.ActivityContextFactory$.withAttributionTags(ActivityContextFactory.scala:53)
	at com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:381)
	at com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:164)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:478)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:375)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:523)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
	at com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:80)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.lang.Thread.run(Thread.java:750)
25/11/26 06:59:53 WARN DriverDaemon: Unexpected exception: java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.spark.sql.internal.SharedState.getSchedulerStats(SharedState.scala:416)
	at org.apache.spark.sql.SQLContext.getSchedulerStats(SQLContext.scala:768)
	at com.databricks.backend.daemon.driver.DriverCorral$.getAutoscalingInfo(DriverCorral.scala:1712)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRPCRequest(DriverCorral.scala:1079)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:1128)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:1126)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:119)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:116)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:24)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:24)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)
	at com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:24)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:24)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:115)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:958)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:958)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:874)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2(JettyServer.scala:503)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2$adapted(JettyServer.scala:478)
	at com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:387)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)
	at com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)
	at com.databricks.logging.activity.ActivityContextFactory$.withAttributionTags(ActivityContextFactory.scala:53)
	at com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:381)
	at com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:164)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:478)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:375)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:523)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
	at com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:80)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.lang.Thread.run(Thread.java:750)
25/11/26 06:59:58 WARN DriverDaemon: Unexpected exception: java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.spark.sql.internal.SharedState.getSchedulerStats(SharedState.scala:416)
	at org.apache.spark.sql.SQLContext.getSchedulerStats(SQLContext.scala:768)
	at com.databricks.backend.daemon.driver.DriverCorral$.getAutoscalingInfo(DriverCorral.scala:1712)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRPCRequest(DriverCorral.scala:1079)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:1128)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:1126)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:119)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:116)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:24)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:24)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)
	at com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:24)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:24)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:115)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:958)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:958)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:874)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2(JettyServer.scala:503)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2$adapted(JettyServer.scala:478)
	at com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:387)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)
	at com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)
	at com.databricks.logging.activity.ActivityContextFactory$.withAttributionTags(ActivityContextFactory.scala:53)
	at com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:381)
	at com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:164)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:478)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:375)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:523)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
	at com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:80)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.lang.Thread.run(Thread.java:750)
